{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from collections import deque\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/eno/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/eno/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing our corpus and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "file_path = 'combined_articles.txt'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unique_words(text):\n",
    "    # Lowercase and tokenize the text\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Count the number of words and unique words\n",
    "    print(\"# of words in the text: \", len(words))\n",
    "\n",
    "    unique_words = sorted(set(words))\n",
    "    print(\"# of unqiue words in the text: \", len(unique_words))\n",
    "\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of words in the text:  259101\n",
      "# of unqiue words in the text:  20355\n"
     ]
    }
   ],
   "source": [
    "unique_words = extract_unique_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbows(text, window_size):\n",
    "    # Lowercase and tokenize the text\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Create CBOW pairs with a given window size\n",
    "    cbows = []\n",
    "    for i, target_word in enumerate(words):\n",
    "        context_words = words[max(0, i - window_size):i] + words[i + 1:i + window_size + 1]\n",
    "        if len(context_words) == window_size * 2:\n",
    "            cbows.append((context_words, target_word))\n",
    "    return cbows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Word: ['china', 'chinese', '中国', 'zhōngguó', 'officially', 'people'], Context Word: pinyin\n",
      "Target Word: ['chinese', '中国', 'pinyin', 'officially', 'people', 'republic'], Context Word: zhōngguó\n",
      "Target Word: ['中国', 'pinyin', 'zhōngguó', 'people', 'republic', 'china'], Context Word: officially\n"
     ]
    }
   ],
   "source": [
    "# Create skip-grams\n",
    "cbows = generate_cbows(text, window_size=3)\n",
    "\n",
    "# Display the results\n",
    "for context_words, target_word in cbows[:3]:\n",
    "    print(f'Target Word: {context_words}, Context Word: {target_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154594"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cbows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, unique_words):\n",
    "    encoding = [1 if word == w else 0 for w in unique_words]\n",
    "    return torch.tensor(encoding, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aaa': tensor([0., 1., 0.,  ..., 0., 0., 0.]),\n",
       " 'aactas': tensor([0., 0., 1.,  ..., 0., 0., 0.]),\n",
       " 'aba': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abacha': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abacus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abadan': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abajo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abandoned': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abandoning': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abandonment': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abangan': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abaya': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abaza': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abazas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abbas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abbasid': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abbasids': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abbreviated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abc': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abd': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdallah': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdeen': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdel': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdelrahman': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdicate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdicated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdicating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdication': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abducted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abduction': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abductions': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abduh': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdul': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdulaziz': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdullah': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdulsalami': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdur': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abdus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abe': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abel': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abeokuta': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aberdeen': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abhijñānaśākuntalam': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abide': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abidesinin': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ability': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abiola': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abitur': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abkhaz': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'able': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aboard': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abolish': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abolished': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abolition': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abolitionism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aboriginal': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aborted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abortive': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aboul': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abound': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'about': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'above': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abraham': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abreu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abroad': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abrogated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abruptly': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absence': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absences': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absent': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absentia': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absolute': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absolutely': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absolutism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absolutists': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absorbed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absorbing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'absorption': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abstained': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abstracionism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abstract': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abstraction': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abstractionism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abubakar': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abuja': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abul': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abundance': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abundant': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abundantly': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abuse': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'abuses': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'academia': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'academic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'academicians': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'academics': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'academies': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'academism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'academy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acadian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acadians': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'académie': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acapulco': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acarajé': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accede': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acceded': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acceding': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accelerate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accelerated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acceleration': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accent': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accents': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accentuated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accenture': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accept': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acceptable': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acceptance': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accepted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accepting': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accepts': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'access': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accessibility': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accessing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accession': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accident': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accidents': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acclaim': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acclaimed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accolades': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accommodate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accommodated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accommodation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accommodations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accompanied': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accompaniment': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accompaniments': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accompany': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accompanying': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accomplished': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accomplishment': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accomplishments': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accord': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accordance': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accorded': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'according': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accordingly': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accords': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'account': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accountability': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accountable': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accounted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accounting': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accounts': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accredited': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accretion': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acculturated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accumulated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accumulates': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accuracy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accurate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accurately': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accusations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'accused': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aceh': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achaeans': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achaemenian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achaemenid': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achaemenids': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achammer': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achebe': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acheulean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achieve': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achieved': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achievement': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achievements': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achieves': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'achieving': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acid': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acitrón': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acknowledge': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acknowledged': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acknowledging': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acolman': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acquire': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acquired': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acquiring': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acquisition': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acquisitions': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acre': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acres': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acrobatic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acrobatics': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acronym': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acropora': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'across': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acs': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'act': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acting': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'action': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actions': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'activating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'active': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actively': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'activism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'activist': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'activists': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'activities': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'activity': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actopan': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actor': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actress': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actresses': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acts': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actual': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'actually': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'acute': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ad': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ada': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adac': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adam': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adams': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adaptation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adaptations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adapted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adat': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'addaura': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'added': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adding': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'addition': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'additional': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'additionally': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'address': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'addressed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'addressing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ade': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adelaide': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adele': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ademoyega': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aden': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adenauer': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adequate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adetiba': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adewale': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adf': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adh': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adhemar': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adhere': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adhered': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adherence': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adherents': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adheres': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adhering': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adidas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adige': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adil': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adirondack': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adjacent': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adjective': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adjunct': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adjusted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adly': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administered': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administering': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administers': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administration': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administrations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administrative': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administratively': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administrator': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'administrators': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admiral': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admiralty': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admiration': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admire': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admired': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admission': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admissions': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admitted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admitting': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'admixture': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adolf': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adolfo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adopt': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adopted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adopting': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adoption': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adria': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adrianople': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adriatic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adult': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adultery': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adults': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advance': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advanced': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advancement': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advancements': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advances': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advancing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advantage': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advantageous': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advantages': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advent': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adventists': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adventure': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adventurers': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adventures': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adversarial': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adversaries': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adversary': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adverse': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adversely': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adversity': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advertising': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advice': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advised': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adviser': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advisor': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advisors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advisory': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advocacy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advocate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advocated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advocates': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'advocating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adyghe': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'adélie': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aegean': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aeolian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aeon': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aerial': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aeronautica': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aeronautics': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aeronomy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aeroplanes': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aerospace': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aesculapian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aesthetic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afanasy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afar': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afc': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affair': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affairs': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affect': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affected': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affecting': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affections': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affiliated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affiliates': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affiliation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affinity': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affirm': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affirmative': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affirmed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affirming': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affirms': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afford': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affordability': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affordable': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afforded': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'affording': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afforestation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afghan': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afghania': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afghanistan': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afghans': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afn': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afonso': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afontova': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aforementioned': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afoxê': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'africa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'african': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'africans': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afrika': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afrin': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afro': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afroasiatic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afrobeat': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afrobeats': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'after': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aftermath': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afternoon': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afterward': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'afterwards': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'again': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'against': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agal': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agama': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agatha': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agave': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'age': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aged': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agencies': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agency': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agenda': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agents': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ages': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agglomerations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aggravated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aggregate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aggregated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aggression': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aggressive': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agha': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aging': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agitating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agitations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agnosticism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agnostics': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ago': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agra': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agrarian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agree': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agreeable': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agreed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agreeing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agreement': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agreements': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agricultural': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agriculture': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agritourism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agua': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aguas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aguleri': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agung': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'agustín': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahead': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahimsa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahly': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmad': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmadinejad': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmadis': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmadiyya': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmadiyyas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmadu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmedabad': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmet': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahmose': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahriman': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahura': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ahvaz': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aichi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aid': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aide': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aided': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aides': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aiding': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aids': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ailing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aim': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aimed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aiming': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aims': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ainu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'air': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airblue': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airborne': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airbus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aircraft': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aired': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airfields': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airlifted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airline': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airlines': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airplane': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airplanes': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airport': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airports': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airs': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airspace': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airtel': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'airways': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aivazovsky': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ajanta': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ajaokuta': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ajayi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ajda': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ajk': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ajudani': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akatsuki': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akbar': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akbaş': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akdoğan': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akhenaten': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akhmatova': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akihito': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akintola': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akitoye': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akkadian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akkas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akkuyu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akp': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akragas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akrotiri': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aksai': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aksiyon': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aksu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akutagawa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akwa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'akşehir': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'al': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alagoas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alaide': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alam': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alamgiri': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alarcón': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alardah': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alarm': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alaska': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alassane': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albanese': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albania': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albanian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albanians': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albany': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albeit': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albernaz': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albert': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alberta': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alberti': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albertine': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alberto': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albertville': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albigensian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alborz': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albrecht': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albuquerque': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'albán': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alcide': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alcohol': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alcoholic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alcântara': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aldo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alegre': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alejandro': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aleksandr': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aleksey': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alemannic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alemán': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alencar': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aleramo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alert': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alessandro': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alestra': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aleutian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alevi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alevis': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alevites': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexander': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexandre': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexandria': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexandropol': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexei': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexeieff': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alexis': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alfa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alfaro': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alfieri': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alfonso': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alfred': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algae': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algal': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algarves': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algeria': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algerian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algerians': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algiers': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algonquian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algonquin': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'algosaibi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alhaji': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ali': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alice': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alien': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alienated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alienating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alienation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alifa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alighieri': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aligned': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alignment': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alignments': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aliko': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alim': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alitalia': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alive': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alkali': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alkaline': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'all': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alla': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allah': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allahu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allama': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allay': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allegations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alleged': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allegedly': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allegiance': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alleging': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alleviation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alley': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allgemeine': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allia': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alliance': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alliances': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allianz': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allied': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allies': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allocate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allocated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allocating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allocations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allow': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allowances': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allowed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allowing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allows': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'allusion': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alluvial': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ally': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'almarai': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'almaz': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'almeida': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'almezmar': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'almighty': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'almonds': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'almost': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alnajdiyah': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alone': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'along': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alongside': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alonso': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alouette': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alpha': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alphabet': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alphabetic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alphabetically': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alpine': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alps': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alrabiah': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'already': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alsace': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alsatian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'also': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alta': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altai': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altaic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altamirano': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altamura': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altare': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alteration': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altered': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alternate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alternated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alternately': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alternating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alternative': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alternatively': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alternatives': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'although': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altitude': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altitudes': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alto': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altogether': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'altruism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aluminum': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alumnus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aluwaisheg': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alvaro': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alvi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'always': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aly': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'alzheimer': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amado': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amalfi': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amalgam': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amalgamating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amalgamation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amami': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amanah': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amapá': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amaral': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amaravati': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amarela': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amaro': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amassing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amaterasu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amateur': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amazigh': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amazon': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amazonas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambassador': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambassadors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambazonia': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambient': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambition': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambitions': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambitious': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ambrose': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amedeo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amenable': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amended': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amending': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amendment': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amendments': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amenemhat': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amenities': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'america': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'american': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'americans': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'americas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amerigo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amerindian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amerindians': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amethyst': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amharic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amid': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amidst': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amin': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amini': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amirkabir': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amjad': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amlo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amma': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amnesty': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amnok': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'among': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amongst': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amordād': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amount': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amounted': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amounting': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amounts': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ampat': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amphibian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amphibians': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ample': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amplified': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amputation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amr': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amsterdam': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amur': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amusement': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'américa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'américo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'amərətāt': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'an': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anadolu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anahita': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analog': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analyse': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analysed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analysis': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analysts': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analytic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analytical': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analyzed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'analyzing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anambra': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anand': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ananta': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anarchism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anastasianism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anastasio': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anatole': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anatolia': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anatolian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anatomically': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anatomy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancestor': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancestors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancestral': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancestries': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancestry': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anchors': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancien': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancient': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancillary': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancona': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ancus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'and': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andalusian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andaman': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anderen': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anderson': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andhra': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andorra': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andrade': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andrea': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andreas': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andrei': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andreu': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andrew': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andrey': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andreyev': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'android': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andrzej': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andré': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andrés': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'andy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anemia': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angara': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angela': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angeles': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angelico': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angell': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angelo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anger': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angered': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anghiari': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angklung': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angkot': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anglesey': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anglican': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anglicisation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anglicised': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anglophone': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anglosphere': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angola': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angora': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angry': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anguilla': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'angular': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anhui': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aniceto': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'animal': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'animals': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'animated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'animation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anime': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'animism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'animist': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'animosity': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anish': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anita': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anjou': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ankara': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ankón': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anna': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annazids': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anne': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annex': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annexation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annexations': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annexe': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annexed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annexing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annibale': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annihilated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annihilation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anniversary': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'announced': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'announcement': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'announcements': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'announcing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annual': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annualised': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annually': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'annulled': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anointing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anonymous': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'another': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ansel': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'answer': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antakya': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antalya': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antarctic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antarctica': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antariksa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anteaters': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antecedents': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anthem': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anthems': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anthology': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anthony': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antibiotics': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anticipating': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anticlerical': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anticline': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antinous': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antioch': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antiochian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antiochus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antiproton': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antiquated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antiquities': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antiquity': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antireligious': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antisemitic': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antisemitism': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antoine': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anton': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antonio': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antonioni': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antony': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'antônio': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anwar': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'any': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anyaene': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anyang': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anymore': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anyone': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anything': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anywhere': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anzac': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anzacs': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anzali': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anzus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'anáhuac': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aoc': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aosta': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'ap': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apa': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apadana': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apart': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apartheid': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apec': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apennine': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apennines': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apex': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aphrodisias': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apocalypse': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apogee': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apollo': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apostasy': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apostle': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apostles': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appalachian': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apparatus': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apparel': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apparent': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apparently': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appeal': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appealed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appeals': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appear': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appearance': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appearances': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appeared': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appearing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appears': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appellate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appellation': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'applauded': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apple': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apples': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appliances': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'applicable': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'application': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'applications': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'applied': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'applies': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appliques': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apply': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appoint': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appointed': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appointees': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appointing': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appointive': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appointment': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appointments': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appoints': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appomattox': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apportioned': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apportionment': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appreciable': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appreciated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appreciative': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apprentice': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apprenticeship': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apprenticeships': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approach': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approached': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approaches': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approaching': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'appropriate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approval': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approved': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approves': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approving': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approximate': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approximated': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'approximately': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apps': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'apricots': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'april': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aqaba': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aquamarine': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " 'aquarium': tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " ...}"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create one-hot encodings for each word\n",
    "one_hot_encodings = {word: one_hot_encoding(word, unique_words) for word in unique_words}\n",
    "one_hot_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert CBOW pairs to vector pairs\n",
    "cbow_vector_pairs = [([one_hot_encodings[word] for word in context_words], one_hot_encodings[target_word]) for context_words, target_word in cbows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.])]"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_vector_pairs[0][0] # contains the 6 context word one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_vector_pairs[0][1] # contains the center word as one-hot encoded vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the context vectors to get a single context vector\n",
    "cbow_vector_pairs = [(torch.sum(torch.stack(context_vectors), dim=0), target_vector) for context_vectors, target_vector in cbow_vector_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0.,  ..., 0., 0., 0.]),\n",
       " tensor([0., 0., 0.,  ..., 0., 0., 0.]))"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_vector_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle pairs before training\n",
    "cbow_vector_pairs = random.sample(cbow_vector_pairs, len(cbow_vector_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveWord2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self, VOCAB_SIZE, VECTOR_DIM, random_seed) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        torch.manual_seed(random_seed)\n",
    "        self.vocab_size = VOCAB_SIZE\n",
    "        self.vector_dim = VECTOR_DIM\n",
    "        self.W1 = nn.Parameter(data=torch.randn(self.vocab_size, self.vector_dim), requires_grad=True)\n",
    "        self.W2 = nn.Parameter(data=torch.randn(self.vector_dim, self.vocab_size), requires_grad=True)\n",
    "\n",
    "    def forward(self, X) -> torch.tensor:\n",
    "        X = X @ self.W1\n",
    "        X = X @ self.W2\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(unique_words)\n",
    "VECTOR_DIM = 5\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveWord2Vec(VOCAB_SIZE, VECTOR_DIM, random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cbow_vector_pairs = cbow_vector_pairs[:int(len(cbow_vector_pairs)*0.9)]\n",
    "val_cbow_vector_pairs = cbow_vector_pairs[int(len(cbow_vector_pairs)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_set, val_set, epochs, learning_rate, verbose=False):\n",
    "\n",
    "    # Create the loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # Create the optimizer object\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    # Log the loss values\n",
    "    train_set_loss_log = []\n",
    "    validation_set_loss_log = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if verbose: print(\"Epoch: \", epoch)\n",
    "        # Training mode on\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "        num_train_batches = 0\n",
    "\n",
    "        for input_vector, target_vector in train_set:\n",
    "\n",
    "            y_train_logits = model(input_vector)\n",
    "            train_loss = loss_fn(y_train_logits, target_vector)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += train_loss.item()\n",
    "            num_train_batches += 1            \n",
    "\n",
    "        # Calculate average training loss for the epoch\n",
    "        average_train_loss = total_train_loss / num_train_batches\n",
    "        train_set_loss_log.append(average_train_loss)\n",
    "\n",
    "        # Eval mode on\n",
    "        model.eval()            \n",
    "        total_validation_loss = 0.0\n",
    "        num_validation_batches = 0\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            for input_vector, target_vector in val_set:\n",
    "                # Evaluate the validation loss\n",
    "                y_val_logits = model(input_vector)\n",
    "                validation_loss = loss_fn(y_val_logits, target_vector)\n",
    "\n",
    "                total_validation_loss += validation_loss.item()\n",
    "                num_validation_batches += 1\n",
    "        \n",
    "        # Calculate average validation loss for the epoch\n",
    "        average_validation_loss = total_validation_loss / num_validation_batches\n",
    "        validation_set_loss_log.append(average_validation_loss) \n",
    "\n",
    "    return model, train_set_loss_log, validation_set_loss_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch:  1\n"
     ]
    }
   ],
   "source": [
    "model, train_set_loss_log, validation_set_loss_log = train_model(model, train_cbow_vector_pairs, val_cbow_vector_pairs, epochs=2, learning_rate=0.005, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_set_loss_log, color='red', label='train_loss')\n",
    "plt.plot(validation_set_loss_log, color='blue', label='validation_loss')\n",
    "\n",
    "plt.title(\"Loss During Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cross Entropy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Vectors\n",
    "params = list(model.parameters())\n",
    "word_vectors = params[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with the same order mapping\n",
    "word_dict = {word: vector for word, vector in zip(unique_words, word_vectors)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    return (v1 @ v2) / (torch.norm(v1) * torch.norm(v2))\n",
    "\n",
    "def most_similar(word, word_dict, top_k=5):\n",
    "    if word not in word_dict:\n",
    "        raise ValueError(f\"{word} not found in the word dictionary.\")\n",
    "\n",
    "    query_vector = word_dict[word]\n",
    "\n",
    "    # Calculate cosine similarity with all other words in the dictionary\n",
    "    similarities = {}\n",
    "    for other_word, other_vector in word_dict.items():\n",
    "        if other_word != word:\n",
    "            similarity = cosine_similarity(query_vector, other_vector)\n",
    "            similarities[other_word] = similarity\n",
    "\n",
    "    # Sort the words by similarity in descending order\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the top-k most similar words\n",
    "    top_similar_words = sorted_similarities[:top_k]\n",
    "\n",
    "    return top_similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([-1.1258, -1.1524, -0.2506, -0.4339,  0.8487]),\n",
       " 'aaa': tensor([ 1.0034, -0.3029, -2.0079, -0.3065, -0.9500]),\n",
       " 'aactas': tensor([0.8513, 0.9443, 0.4473, 0.8879, 1.4313]),\n",
       " 'aba': tensor([ 0.6022, -0.2317, -2.2191, -0.6540, -0.5188]),\n",
       " 'abacha': tensor([ 0.4889, -0.2688, -0.3681,  0.9355,  1.0961]),\n",
       " 'abacus': tensor([-0.8971, -0.7074,  0.4995,  1.6973,  1.2691]),\n",
       " 'abadan': tensor([ 0.9463, -0.2081,  0.0211,  0.6666, -0.4914]),\n",
       " 'abajo': tensor([ 0.2756,  0.3381, -0.4398,  0.6006,  0.1511]),\n",
       " 'abandoned': tensor([-0.3704, -0.5691,  0.1732, -0.2562,  0.4163]),\n",
       " 'abandoning': tensor([ 1.9956, -1.7834, -0.3168, -0.0597, -0.3850]),\n",
       " 'abandonment': tensor([ 0.1024, -0.1393,  0.4223,  0.5612,  0.2158]),\n",
       " 'abangan': tensor([-0.1411,  0.0525,  0.3468,  1.2792,  3.0063]),\n",
       " 'abate': tensor([-1.0762, -0.9224,  1.4979, -0.6045, -0.9474]),\n",
       " 'abaya': tensor([-0.5966,  1.0587,  1.0482,  0.0663,  2.0810]),\n",
       " 'abaza': tensor([-0.7900,  0.3363, -0.9951, -0.2328, -0.7978]),\n",
       " 'abazas': tensor([-1.1155, -0.5536,  0.4961,  0.1548,  0.2121]),\n",
       " 'abbas': tensor([-0.4380,  0.0296, -1.3868,  0.2803,  2.0263]),\n",
       " 'abbasid': tensor([ 0.1464,  0.3826, -0.1797, -0.3905,  0.5364]),\n",
       " 'abbasids': tensor([-1.8792, -0.2539, -0.1323, -0.9573,  0.3962]),\n",
       " 'abbreviated': tensor([ 1.0915,  0.7744, -1.4421, -0.2062,  0.6003]),\n",
       " 'abc': tensor([-1.7031, -1.5335, -0.9605, -0.3112, -0.9131]),\n",
       " 'abd': tensor([ 0.3373,  0.1469, -1.2293, -0.0131, -0.3176]),\n",
       " 'abdallah': tensor([ 0.1460, -0.2915,  0.0988, -1.4821, -0.7083]),\n",
       " 'abdeen': tensor([-0.9838, -0.2862,  0.3525,  0.0270, -0.3731]),\n",
       " 'abdel': tensor([-0.5945,  0.9121, -0.6646, -0.0697,  0.4590]),\n",
       " 'abdelrahman': tensor([-0.4184,  1.2417,  1.3871,  0.7004, -0.7127]),\n",
       " 'abdicate': tensor([-0.0632,  0.4733,  2.2550, -0.4656,  2.9941]),\n",
       " 'abdicated': tensor([-1.5119, -1.3537, -0.0776, -0.3238, -0.2393]),\n",
       " 'abdicating': tensor([ 0.2155, -1.3512, -0.9479,  0.2458, -0.8949]),\n",
       " 'abdication': tensor([-0.6919,  1.1657,  0.5907, -0.0865, -1.1170]),\n",
       " 'abdu': tensor([-0.4712,  0.3334,  1.2852, -0.5054, -2.1879]),\n",
       " 'abducted': tensor([-0.1828, -0.3332, -0.1733,  0.5040,  0.3543]),\n",
       " 'abduction': tensor([ 0.0970, -1.4239,  1.0244,  0.2042,  0.9518]),\n",
       " 'abductions': tensor([-0.4685,  0.5023,  1.0163,  0.2299,  0.0966]),\n",
       " 'abduh': tensor([ 0.8330,  0.3637, -0.9481, -1.7142,  1.1936]),\n",
       " 'abdul': tensor([-1.2117, -0.9842,  0.6610, -0.4043,  0.3587]),\n",
       " 'abdulaziz': tensor([ 0.4740,  0.0319, -0.3112, -0.3758, -0.9606]),\n",
       " 'abdullah': tensor([ 1.1607, -0.2254,  0.7758,  0.4580,  1.0378]),\n",
       " 'abdulsalami': tensor([-1.5070, -1.5135, -0.2419,  0.2158,  0.0893]),\n",
       " 'abdur': tensor([ 0.4245,  1.1243,  0.2279, -0.6052,  0.7041]),\n",
       " 'abdus': tensor([ 0.2481, -1.7157,  0.0899, -0.0487,  0.3464]),\n",
       " 'abe': tensor([-0.8927,  0.5078, -0.5313,  0.8550,  1.0464]),\n",
       " 'abel': tensor([ 0.0156,  0.5279, -0.6236,  1.9450, -1.1351]),\n",
       " 'abeokuta': tensor([-1.9697,  1.2747,  0.8430, -0.2273, -0.0111]),\n",
       " 'aberdeen': tensor([ 0.3785, -0.1386, -0.1452, -0.1212,  0.1771]),\n",
       " 'abhijñānaśākuntalam': tensor([ 1.2751, -0.5617,  0.1225,  0.0504, -0.4140]),\n",
       " 'abi': tensor([-0.0683, -1.0130,  0.7649, -0.8465, -0.2072]),\n",
       " 'abide': tensor([ 0.8631, -1.2590,  0.7354,  1.1483, -2.4833]),\n",
       " 'abidesinin': tensor([-2.2237, -1.1419, -0.6428, -0.5572,  0.6989]),\n",
       " 'ability': tensor([-0.8237,  1.2566,  0.4513, -0.2628,  0.3834]),\n",
       " 'abiola': tensor([-0.2917, -0.0391, -0.5282,  1.3254,  0.9963]),\n",
       " 'abitur': tensor([ 0.1152,  0.2782, -1.4355, -0.3395, -0.9968]),\n",
       " 'abkhaz': tensor([ 0.5719, -0.8607,  0.4870, -0.1465,  0.3425]),\n",
       " 'able': tensor([0.7753, 1.1372, 0.6783, 0.0447, 0.2363]),\n",
       " 'aboard': tensor([ 0.6908,  1.8108, -1.8013,  0.5404, -0.5122]),\n",
       " 'abolish': tensor([-0.1729, -0.9683,  0.6631, -0.7921,  0.7209]),\n",
       " 'abolished': tensor([-1.5940, -0.4962, -1.0860, -0.5528,  0.3713]),\n",
       " 'abolition': tensor([-0.2734,  0.7820, -0.6904, -0.0971, -0.5414]),\n",
       " 'abolitionism': tensor([-0.4994, -1.3627, -0.1067, -0.8881, -0.1978]),\n",
       " 'aboriginal': tensor([-0.5926,  0.2515,  0.4942,  0.2111, -0.1540]),\n",
       " 'aborted': tensor([ 1.1457, -0.6686,  0.6955, -0.1157, -0.3146]),\n",
       " 'abortive': tensor([-0.1331, -0.9366,  1.3609,  0.5160, -0.3286]),\n",
       " 'aboul': tensor([ 0.9663, -1.2898, -0.6047,  0.0063, -0.6745]),\n",
       " 'abound': tensor([-1.3317,  1.5493, -0.0094, -0.8856, -0.5704]),\n",
       " 'about': tensor([-0.1292, -0.0546,  0.4083,  1.1264,  1.9351]),\n",
       " 'above': tensor([ 1.0077,  1.0046, -0.4335, -1.2426,  1.2846]),\n",
       " 'abraham': tensor([-0.0479,  0.7085, -0.4591, -2.1543,  1.2660]),\n",
       " 'abreu': tensor([-1.8539,  0.6660,  3.2197,  0.8284, -0.0225]),\n",
       " 'abroad': tensor([ 0.1568,  0.7307,  1.3412, -0.2358,  0.3291]),\n",
       " 'abrogated': tensor([-1.7314, -1.4161, -0.9188, -0.2672, -0.2645]),\n",
       " 'abruptly': tensor([ 2.5616, -0.5754, -0.2282,  0.6460,  0.8648]),\n",
       " 'absence': tensor([ 1.1367, -0.0080,  0.8217,  0.5509,  1.1242]),\n",
       " 'absences': tensor([ 1.1892,  0.3800,  1.0867, -1.2855, -0.2572]),\n",
       " 'absent': tensor([ 0.0025, -0.6531, -0.1802,  0.4963, -0.0775]),\n",
       " 'absentia': tensor([ 0.3887,  0.8119,  0.6291, -0.5331, -1.0009]),\n",
       " 'absolute': tensor([-0.1632,  0.4303, -1.4752, -0.8614,  0.1706]),\n",
       " 'absolutely': tensor([-0.5464,  0.4082, -0.7271,  0.3082,  0.0557]),\n",
       " 'absolutism': tensor([ 0.5062,  0.9573,  0.0616,  0.8384, -1.6833]),\n",
       " 'absolutists': tensor([ 0.1042, -0.0423,  1.4917,  3.3999,  1.2011]),\n",
       " 'absorbed': tensor([-0.7988, -0.3720, -0.1447, -0.1860, -1.1936]),\n",
       " 'absorbing': tensor([-1.0885, -0.0832,  0.1728, -1.2852,  1.0652]),\n",
       " 'absorption': tensor([-0.8668,  1.7882, -0.7720,  2.4740, -1.1673]),\n",
       " 'abstained': tensor([ 1.3708,  1.1963, -0.6642, -1.2953,  2.1693]),\n",
       " 'abstracionism': tensor([ 1.4675,  0.6235,  1.2973, -1.3819,  1.7720]),\n",
       " 'abstract': tensor([-1.4135,  0.8763, -0.8369,  0.4603,  0.9026]),\n",
       " 'abstraction': tensor([ 1.4698,  0.4385,  2.7988, -0.0061,  0.0054]),\n",
       " 'abstractionism': tensor([ 1.6000, -0.8761,  0.9851,  0.5935, -0.6033]),\n",
       " 'abu': tensor([-0.3946,  0.9133, -0.1330, -0.7105, -0.5673]),\n",
       " 'abubakar': tensor([-0.7218, -0.6565, -0.1150, -0.0998,  0.0678]),\n",
       " 'abuja': tensor([ 0.1388,  1.0643, -0.9016, -1.1500, -0.3294]),\n",
       " 'abul': tensor([ 0.8497, -0.4232,  3.0849,  0.0663, -0.1582]),\n",
       " 'abundance': tensor([ 1.6024,  0.1305, -0.2982,  0.3152,  0.8315]),\n",
       " 'abundant': tensor([ 1.7955, -0.4102, -1.0082,  0.3219, -1.2131]),\n",
       " 'abundantly': tensor([-0.6939, -0.8738, -0.3599, -0.0811,  0.1925]),\n",
       " 'abuse': tensor([ 0.4220, -0.4758, -1.4123, -0.4793,  0.4256]),\n",
       " 'abuses': tensor([ 0.5955,  0.0914, -1.1997, -0.1379, -0.3264]),\n",
       " 'academia': tensor([-1.4052,  0.3773, -0.4615, -0.9102,  0.1802]),\n",
       " 'academic': tensor([ 1.2110,  0.5628,  1.2179, -0.5566,  0.2479]),\n",
       " 'academicians': tensor([ 0.2712, -0.6859, -0.4562,  1.6797,  0.3898]),\n",
       " 'academics': tensor([-1.0489,  0.0885,  0.4165,  0.1742, -0.4781]),\n",
       " 'academies': tensor([-0.0322, -0.2974, -1.5952,  0.0745, -0.4526]),\n",
       " 'academism': tensor([ 1.9873, -0.7298, -1.4567, -0.7336, -2.3342]),\n",
       " 'academy': tensor([ 0.2736,  0.0495,  0.3860, -0.6830,  0.7023]),\n",
       " 'acadian': tensor([ 0.3394,  0.2099,  0.0298,  1.7092, -0.4081]),\n",
       " 'acadians': tensor([-0.1373,  0.2793,  0.3358,  0.5385,  0.1062]),\n",
       " 'académie': tensor([ 0.8005,  1.0824, -0.3305, -0.9826,  0.5246]),\n",
       " 'acapulco': tensor([-1.4977, -0.4243, -0.6482, -0.1256, -0.8104]),\n",
       " 'acarajé': tensor([ 0.8890,  0.6858,  1.6357,  2.1583, -1.9953]),\n",
       " 'accede': tensor([ 1.1455, -1.5523,  0.6509, -0.0984,  1.2906]),\n",
       " 'acceded': tensor([-0.0557,  0.7778, -1.0474, -0.3542, -0.9730]),\n",
       " 'acceding': tensor([ 0.8589, -0.2496,  0.0902,  1.2123, -0.3572]),\n",
       " 'accelerate': tensor([ 0.7588,  0.3208,  1.1756, -0.2418,  0.7354]),\n",
       " 'accelerated': tensor([ 0.9734,  1.2005, -0.2500,  0.1057, -0.8737]),\n",
       " 'acceleration': tensor([ 1.3145, -1.3006,  0.8765, -0.4160, -0.3552]),\n",
       " 'accent': tensor([ 0.0651, -1.9967,  0.4970,  1.9926, -0.3957]),\n",
       " 'accents': tensor([-0.9825, -0.1398,  0.0038, -1.1523, -1.0850]),\n",
       " 'accentuated': tensor([1.6797, 0.5340, 0.2365, 0.4080, 0.6084]),\n",
       " 'accenture': tensor([ 0.8653, -0.2941, -2.7201, -0.2879, -0.6365]),\n",
       " 'accept': tensor([-0.0030,  0.1283,  0.8886, -1.0077,  1.2471]),\n",
       " 'acceptable': tensor([ 1.5239,  0.0123, -0.2945, -0.0885,  0.1127]),\n",
       " 'acceptance': tensor([ 0.7715,  1.2832,  1.2155, -0.2011,  0.3671]),\n",
       " 'accepted': tensor([ 0.9651, -0.8192, -1.3251, -0.2222, -0.2972]),\n",
       " 'accepting': tensor([ 0.4537, -0.3583,  0.1047,  0.6836, -0.6420]),\n",
       " 'accepts': tensor([ 1.3777, -0.7618,  0.1580,  0.4572,  0.4368]),\n",
       " 'access': tensor([ 0.9164,  0.0373,  0.4253, -0.3086,  0.1008]),\n",
       " 'accessibility': tensor([ 0.3257, -0.8362,  0.1190,  0.5154,  1.8477]),\n",
       " 'accessing': tensor([ 0.6751, -0.6105, -0.7973, -0.4872, -1.5396]),\n",
       " 'accession': tensor([-0.2872, -0.5765, -0.6817, -0.0334,  0.2204]),\n",
       " 'accident': tensor([ 0.7693,  0.0964,  0.1527, -0.7209, -0.6251]),\n",
       " 'accidents': tensor([0.8434, 1.3215, 1.0008, 0.6516, 0.5666]),\n",
       " 'acclaim': tensor([ 1.0513,  2.1051, -0.4861, -0.2163,  2.3061]),\n",
       " 'acclaimed': tensor([ 0.8610, -0.2443,  0.5516,  0.6392,  0.0199]),\n",
       " 'accolades': tensor([ 0.3977,  0.5002,  1.6549, -1.3309, -0.7513]),\n",
       " 'accommodate': tensor([-0.1709, -0.2900,  0.6312, -0.0344, -0.1216]),\n",
       " 'accommodated': tensor([ 0.5237,  0.8490,  0.9544,  0.0977, -0.5022]),\n",
       " 'accommodation': tensor([-2.9476, -0.5865,  0.5172, -0.7011,  1.5039]),\n",
       " 'accommodations': tensor([-0.6260,  1.1746,  0.3016,  0.3756, -1.4301]),\n",
       " 'accompanied': tensor([-0.4429, -0.1627,  0.3235,  0.5221,  0.5613]),\n",
       " 'accompaniment': tensor([-1.7845,  0.0247, -1.0821, -0.2948,  0.2498]),\n",
       " 'accompaniments': tensor([-0.9197, -0.6040,  0.8495, -0.8379, -0.8446]),\n",
       " 'accompany': tensor([ 1.3222, -1.9586,  0.5822,  0.3680,  0.0662]),\n",
       " 'accompanying': tensor([ 1.0501,  1.9208, -0.9821, -0.7683, -1.8554]),\n",
       " 'accomplished': tensor([0.1272, 0.1246, 0.2779, 0.7638, 0.1424]),\n",
       " 'accomplishment': tensor([ 1.2928, -0.7570, -1.7153, -0.2695, -1.2996]),\n",
       " 'accomplishments': tensor([ 0.8082,  0.6845, -0.2203,  0.3498, -0.5307]),\n",
       " 'accord': tensor([-1.5718,  2.3597,  0.0924, -2.1111,  0.6297]),\n",
       " 'accordance': tensor([-0.5374, -0.4689,  0.7712,  0.1672,  0.5421]),\n",
       " 'accorded': tensor([ 0.1517, -1.4432,  0.3106,  0.4201,  0.2533]),\n",
       " 'according': tensor([ 0.8742,  0.1851,  0.2923, -0.0573, -0.0615]),\n",
       " 'accordingly': tensor([ 0.0267, -0.7160,  0.3620,  0.1625,  1.4633]),\n",
       " 'accords': tensor([ 0.7884, -0.3648, -0.3378, -0.0798,  0.6592]),\n",
       " 'account': tensor([ 1.8427,  0.3958, -0.2052, -0.1831, -0.7763]),\n",
       " 'accountability': tensor([-0.1168, -1.2568,  0.3413, -0.9024,  0.0376]),\n",
       " 'accountable': tensor([1.1012, 0.5973, 0.3338, 0.5807, 0.0327]),\n",
       " 'accounted': tensor([ 2.2773, -0.2038, -0.9120, -0.2859, -0.1102]),\n",
       " 'accounting': tensor([ 0.8748,  0.8938, -0.0277, -0.1223,  0.0070]),\n",
       " 'accounts': tensor([ 1.5399, -0.4332, -1.0123, -0.7038, -1.0310]),\n",
       " 'accredited': tensor([-1.1912, -0.2370,  0.4849,  1.1328,  1.6208]),\n",
       " 'accretion': tensor([-0.3907, -1.5676, -0.1104, -1.4619,  1.1676]),\n",
       " 'acculturated': tensor([-0.1622,  1.1140,  1.1872,  0.2470,  0.1543]),\n",
       " 'accumulated': tensor([-1.1165, -0.3504, -0.3377, -0.6330,  0.7789]),\n",
       " 'accumulates': tensor([ 1.0891, -2.3429, -0.4614, -0.2151,  0.0203]),\n",
       " 'accuracy': tensor([ 2.2887,  0.5865, -0.3063,  1.5201, -1.4463]),\n",
       " 'accurate': tensor([-2.1180, -1.3288, -1.4209,  0.0522, -1.4486]),\n",
       " 'accurately': tensor([-1.4847, -1.0468,  2.4401, -1.4310,  1.0386]),\n",
       " 'accusations': tensor([ 0.5566,  0.2237,  0.2442, -2.1160, -0.4254]),\n",
       " 'accused': tensor([ 0.7331,  0.4531,  0.8876, -0.5591,  0.1279]),\n",
       " 'aceh': tensor([ 0.2633, -1.2229, -0.6528, -0.6936, -0.1081]),\n",
       " 'achaeans': tensor([-0.0144, -0.2076,  0.1681, -0.7484, -1.9405]),\n",
       " 'achaemenian': tensor([ 0.7953, -1.6042, -0.7183, -0.9126, -0.2005]),\n",
       " 'achaemenid': tensor([-0.4488,  1.2118, -0.1451, -0.7056, -0.1289]),\n",
       " 'achaemenids': tensor([-0.1709, -0.1944, -0.0027, -1.3225,  0.6533]),\n",
       " 'achammer': tensor([-0.8367, -0.0282, -1.4937,  0.4301,  1.4765]),\n",
       " 'achebe': tensor([-0.8920, -0.3945, -0.7220,  0.8215,  0.4458]),\n",
       " 'acheulean': tensor([-0.1865, -2.0268, -0.9086,  0.2175, -0.2675]),\n",
       " 'achieve': tensor([ 0.8797, -0.0046, -0.6010, -0.2534,  0.7416]),\n",
       " 'achieved': tensor([-0.8376,  1.3788, -0.3635,  0.1485, -0.1215]),\n",
       " 'achievement': tensor([ 0.1741,  0.0812, -0.1378, -0.1129,  0.2894]),\n",
       " 'achievements': tensor([ 0.1945, -0.0531,  0.8137, -1.0603,  0.2954]),\n",
       " 'achieves': tensor([-0.5539, -0.0997, -1.3270,  1.2890, -0.9013]),\n",
       " 'achieving': tensor([-1.5667, -0.2410, -0.7481, -0.9106,  0.1007]),\n",
       " 'acid': tensor([ 0.3297, -0.4377, -0.0669,  0.4482,  2.1335]),\n",
       " 'acitrón': tensor([-1.2347, -2.5465, -0.1199,  0.1395,  1.9532]),\n",
       " 'acknowledge': tensor([ 0.4709,  0.1328, -0.5142, -1.4552,  0.7232]),\n",
       " 'acknowledged': tensor([-2.2765, -1.3466,  0.0878, -0.3656,  1.9112]),\n",
       " 'acknowledging': tensor([-0.3292, -1.5233,  0.7676,  1.5185,  0.5125]),\n",
       " 'acolman': tensor([ 1.0300, -0.2489,  0.2623,  1.9892, -1.3557]),\n",
       " 'acquire': tensor([-1.8553, -1.1493,  0.6963, -2.1385,  0.0207]),\n",
       " 'acquired': tensor([ 0.0413, -0.4009,  0.2913, -1.0016, -0.4738]),\n",
       " 'acquiring': tensor([-0.3988, -0.5516, -0.8429, -0.0191, -0.0146]),\n",
       " 'acquisition': tensor([-0.2872,  0.0025,  1.0433,  0.2365, -0.2886]),\n",
       " 'acquisitions': tensor([-0.5611,  0.1092, -0.2231,  0.1745, -0.2187]),\n",
       " 'acre': tensor([-0.5396,  1.3088, -1.3477,  0.4330,  0.8787]),\n",
       " 'acres': tensor([ 0.4446, -0.6192, -0.3453,  0.0875, -0.2989]),\n",
       " 'acrobatic': tensor([-1.4208, -0.3986,  0.6461, -0.8013,  1.3698]),\n",
       " 'acrobatics': tensor([-0.1772, -2.1500,  0.7573,  0.3684, -1.6602]),\n",
       " 'acronym': tensor([-0.4615, -0.2795, -0.3711,  0.5548, -0.4051]),\n",
       " 'acropora': tensor([ 1.1731, -0.5802,  0.4497,  1.7659,  0.1781]),\n",
       " 'across': tensor([ 0.0929,  0.5673, -0.1805, -0.2210, -1.8014]),\n",
       " 'acs': tensor([ 1.3713, -0.0947, -0.0936,  0.6798,  0.5262]),\n",
       " 'act': tensor([-0.6498, -0.6824, -0.3470, -0.1430,  0.2566]),\n",
       " 'acting': tensor([-0.0920, -0.5847, -0.3721,  0.0308, -0.1530]),\n",
       " 'action': tensor([ 1.2615, -1.7496, -1.1394,  0.0438,  1.2248]),\n",
       " 'actions': tensor([-0.1990, -0.2179, -0.9911,  0.4830,  0.7581]),\n",
       " 'activating': tensor([-1.9520, -0.1012,  0.4450, -1.0827, -0.4491]),\n",
       " 'active': tensor([ 0.1604, -0.3779, -0.4130,  0.3773,  0.0642]),\n",
       " 'actively': tensor([ 0.1924,  0.7945,  0.7458, -0.3166,  0.0394]),\n",
       " 'activism': tensor([-0.1222, -0.0520,  1.3586, -0.0547, -0.7118]),\n",
       " 'activist': tensor([ 0.7194,  0.5995, -1.1973, -0.3347, -0.1366]),\n",
       " 'activists': tensor([ 0.7731,  0.4120, -0.8653,  0.2945, -0.7474]),\n",
       " 'activities': tensor([ 0.1749,  0.1312,  0.2421, -0.6990,  0.4836]),\n",
       " 'activity': tensor([-0.7225,  0.4346, -0.6433, -0.5020,  0.5678]),\n",
       " 'actopan': tensor([-1.1622,  1.2376, -1.2270, -1.4513,  1.7069]),\n",
       " 'actor': tensor([-0.1398, -0.0414,  0.3628,  0.4721, -1.2511]),\n",
       " 'actors': tensor([-0.8503,  0.8000, -0.5020, -0.4787, -0.1249]),\n",
       " 'actress': tensor([-0.2015,  0.2668,  0.7784, -0.2919, -1.2288]),\n",
       " 'actresses': tensor([ 0.6289,  0.7142, -0.4038, -1.5053,  1.1839]),\n",
       " 'acts': tensor([-2.0615, -0.1466,  0.4475,  0.0572, -0.2054]),\n",
       " 'actual': tensor([-0.5589, -0.8211, -0.4928, -1.1603,  0.3785]),\n",
       " 'actually': tensor([ 0.0155, -0.0551,  1.0674,  0.2333, -0.8799]),\n",
       " 'acute': tensor([ 0.3310,  0.2963,  0.6287, -0.1788,  0.5797]),\n",
       " 'ad': tensor([-0.8010,  1.3402, -0.1819, -0.0611,  0.3597]),\n",
       " 'ada': tensor([ 0.2115,  0.4664,  0.2595,  0.0309, -1.9181]),\n",
       " 'adac': tensor([-0.9150,  0.3884, -0.0255,  1.2518,  0.1612]),\n",
       " 'adam': tensor([-0.8954,  0.1688, -0.2459,  0.8827, -0.0748]),\n",
       " 'adams': tensor([-0.2006, -0.1870, -0.5499,  0.1583,  0.8941]),\n",
       " 'adaptation': tensor([ 0.2789, -0.7650,  0.6205,  0.1442, -1.3700]),\n",
       " 'adaptations': tensor([ 0.5761, -0.0578,  1.1714, -1.5184, -0.5877]),\n",
       " 'adapted': tensor([ 0.0418, -0.9238,  0.1019, -0.4864,  0.2246]),\n",
       " 'adat': tensor([ 0.0348, -1.1063,  0.4427, -1.3120,  0.4432]),\n",
       " 'addaura': tensor([-0.9103,  0.4154,  0.4665, -0.4021,  0.8266]),\n",
       " 'added': tensor([ 0.6500,  0.5196, -0.4003, -0.0392,  0.1353]),\n",
       " 'adding': tensor([-0.6502,  0.1146,  0.8315, -0.1257, -0.2452]),\n",
       " 'addition': tensor([ 0.4404, -0.2790,  0.1879, -0.2654, -1.1335]),\n",
       " 'additional': tensor([ 0.7248, -1.4584, -0.2512,  0.0869, -0.1257]),\n",
       " 'additionally': tensor([ 0.4692,  0.6478,  0.6529, -0.4237,  0.1167]),\n",
       " 'address': tensor([ 0.2737,  0.8155,  0.5873, -0.3891,  1.5855]),\n",
       " 'addressed': tensor([ 0.5268, -0.9326,  0.5372,  0.9650, -0.2255]),\n",
       " 'addressing': tensor([-0.7465, -1.4738,  0.5504,  0.4612,  0.9796]),\n",
       " 'ade': tensor([ 0.5307, -1.7448,  0.0046,  0.1292,  0.6072]),\n",
       " 'adelaide': tensor([-1.6592, -0.3600, -0.8265,  0.4792,  0.5422]),\n",
       " 'adele': tensor([-0.8136, -0.1953,  2.7468,  1.7015,  1.3863]),\n",
       " 'ademoyega': tensor([ 3.2437,  0.5252, -0.9332, -0.3479, -0.1436]),\n",
       " 'aden': tensor([-0.1880, -0.5740, -0.0028,  0.3147, -0.8969]),\n",
       " 'adenauer': tensor([ 1.6530,  0.1087,  0.1793, -0.0186,  0.5256]),\n",
       " 'adequate': tensor([-0.0646,  0.2599, -2.1338, -1.8786,  1.6915]),\n",
       " 'adetiba': tensor([-0.1645,  0.7290,  1.2697,  0.1190, -0.8658]),\n",
       " 'adewale': tensor([ 0.4679, -0.0387,  1.3356,  1.1550, -0.3740]),\n",
       " 'adf': tensor([ 0.5230, -0.3894, -0.9616,  0.0114,  1.2784]),\n",
       " 'adh': tensor([1.1587, 0.6758, 0.6278, 0.8808, 0.6279]),\n",
       " 'adhemar': tensor([0.3451, 1.3332, 1.5625, 0.3171, 0.8585]),\n",
       " 'adhere': tensor([-0.3301, -0.6696, -0.1186, -0.6353, -2.0506]),\n",
       " 'adhered': tensor([ 0.5569,  0.5581, -0.5807, -0.1636,  0.8725]),\n",
       " 'adherence': tensor([ 0.9231, -0.6467, -1.0398,  1.1845,  0.3346]),\n",
       " 'adherents': tensor([ 0.7747,  0.2281,  0.1683, -0.3442, -0.3293]),\n",
       " 'adheres': tensor([ 2.4342, -1.3318, -2.5013, -0.6508, -2.0648]),\n",
       " 'adhering': tensor([-0.0278,  0.9378,  0.6930,  0.1130, -0.9315]),\n",
       " 'adi': tensor([-1.0088,  0.8987,  1.3315,  0.4055,  0.7949]),\n",
       " 'adidas': tensor([-1.7380, -0.2445,  0.4427,  1.7678,  0.6801]),\n",
       " 'adige': tensor([ 1.4433,  1.1626, -0.7735, -0.2915, -1.2713]),\n",
       " 'adil': tensor([-0.5842, -0.2285,  2.3096, -0.2051, -1.8604]),\n",
       " 'adirondack': tensor([-0.1806,  1.8160,  0.8276, -0.6056,  0.4621]),\n",
       " 'adjacent': tensor([ 0.9252,  0.3472, -1.2329, -1.2490, -1.4737]),\n",
       " 'adjective': tensor([-0.0496,  0.1602,  0.0743, -1.9999,  0.2399]),\n",
       " 'adjunct': tensor([-0.4127,  0.0289,  0.3514,  2.1300, -0.9283]),\n",
       " 'adjusted': tensor([ 0.1369,  0.4201,  0.6410, -0.4722,  0.0331]),\n",
       " 'adly': tensor([-0.9042,  0.3117,  1.3782,  0.0384, -0.6390]),\n",
       " 'administered': tensor([-0.0800, -0.9243,  1.2585, -0.3065, -0.3048]),\n",
       " 'administering': tensor([ 0.9555, -0.8686,  0.2182,  0.2977,  1.4283]),\n",
       " 'administers': tensor([ 0.8206, -1.4244, -0.0263, -0.6215, -0.8577]),\n",
       " 'administration': tensor([-0.0709, -1.5976,  0.2359, -0.8715,  0.9888]),\n",
       " 'administrations': tensor([-0.7905,  0.1250, -0.1779, -0.9310, -0.2717]),\n",
       " 'administrative': tensor([-0.8217, -0.5315, -0.7305, -0.1461,  1.0656]),\n",
       " 'administratively': tensor([-0.7679, -1.4528,  0.2426, -0.4942, -0.5805]),\n",
       " 'administrator': tensor([ 0.3270, -2.1944,  0.4117,  0.7209, -0.3548]),\n",
       " 'administrators': tensor([-0.4485,  1.8043,  0.0794, -1.0797,  0.6416]),\n",
       " 'admiral': tensor([ 1.6214,  0.0764, -0.1629,  0.7095,  0.7448]),\n",
       " 'admiralty': tensor([ 0.6297, -0.2248,  0.6016,  1.4347,  0.1507]),\n",
       " 'admiration': tensor([-0.7101, -0.1565, -1.6892,  1.6517,  1.6903]),\n",
       " 'admire': tensor([ 0.7182, -0.7989, -0.8015, -0.0631,  0.3516]),\n",
       " 'admired': tensor([-0.7893,  1.8439,  1.3151, -0.5604, -0.4962]),\n",
       " 'admission': tensor([ 0.1576,  0.1321, -0.3605,  0.2203,  1.1086]),\n",
       " 'admissions': tensor([ 1.8802, -0.5267, -1.0555,  0.4307, -0.4120]),\n",
       " 'admitted': tensor([ 1.0863,  0.3635, -0.6374, -0.2784,  1.2725]),\n",
       " 'admitting': tensor([ 0.7686,  2.0455,  1.7358, -0.2430,  1.0779]),\n",
       " 'admixture': tensor([-0.3705,  0.2650,  0.1635, -1.0692, -0.3500]),\n",
       " 'adolf': tensor([0.5013, 1.3432, 0.4515, 0.3882, 2.0991]),\n",
       " 'adolfo': tensor([ 2.1866,  0.2522, -0.1969, -0.7761, -0.5363]),\n",
       " 'adopt': tensor([ 0.7503,  1.5530,  0.2105, -1.0962,  1.4248]),\n",
       " 'adopted': tensor([ 0.0219, -0.1359,  0.3240, -0.3948,  0.2822]),\n",
       " 'adopting': tensor([-0.1981, -0.3109, -1.0421,  0.2878,  1.3937]),\n",
       " 'adoption': tensor([ 0.9988,  0.0666,  0.6864, -0.2219,  0.8805]),\n",
       " 'adria': tensor([ 0.4639,  0.1023, -0.9330,  0.4310,  0.7995]),\n",
       " 'adrianople': tensor([ 0.7823, -0.2188, -1.2465, -0.4748,  0.3044]),\n",
       " 'adriatic': tensor([ 0.9571,  0.9099, -0.7184,  0.0528, -1.4373]),\n",
       " 'adult': tensor([ 1.4453, -0.0597, -0.0189,  0.2390,  0.0456]),\n",
       " 'adultery': tensor([-0.1761, -1.5641,  0.8495,  0.4936, -0.5132]),\n",
       " 'adults': tensor([ 7.6015e-01, -7.9775e-01,  8.1346e-01, -4.0433e-02,  4.2283e-04]),\n",
       " 'advance': tensor([ 0.2416, -0.1850,  0.3915,  0.6127, -0.9000]),\n",
       " 'advanced': tensor([ 0.8515, -0.1062,  0.1733, -0.1944, -0.1055]),\n",
       " 'advancement': tensor([ 0.3460,  0.9317, -1.6369, -0.2265, -0.6154]),\n",
       " 'advancements': tensor([ 0.9144, -0.5307,  1.6708, -1.2899, -1.4405]),\n",
       " 'advances': tensor([-0.1660,  0.8665,  1.6096, -1.1344,  0.2483]),\n",
       " 'advancing': tensor([ 0.9547, -0.2049, -0.0835, -0.9407,  1.0285]),\n",
       " 'advantage': tensor([-0.5836,  0.6847, -0.3417, -1.0834, -0.6486]),\n",
       " 'advantageous': tensor([-2.6307,  0.4513,  0.0049,  1.1765,  1.0584]),\n",
       " 'advantages': tensor([-0.4313,  1.0154, -0.5273, -0.0363,  0.0990]),\n",
       " 'advent': tensor([-0.7428, -0.5896,  0.9524, -0.4707, -0.7452]),\n",
       " 'adventists': tensor([-0.6879, -0.3380, -0.2763, -0.5416,  0.2293]),\n",
       " 'adventure': tensor([ 0.9776,  1.4642,  2.1254,  1.1569, -0.4915]),\n",
       " 'adventurers': tensor([ 0.8561, -1.3487, -1.8309, -0.5768,  0.0422]),\n",
       " 'adventures': tensor([ 0.9817,  0.5105,  0.5333, -0.0031,  0.2507]),\n",
       " 'adversarial': tensor([-0.5622, -0.4986,  0.5166, -0.8905, -0.1954]),\n",
       " 'adversaries': tensor([ 0.1321,  0.7007,  0.5417,  0.1072, -0.1061]),\n",
       " 'adversary': tensor([-0.2506,  0.1202, -2.6213, -0.6298,  0.6108]),\n",
       " 'adverse': tensor([ 0.0169, -1.0239,  1.6858,  0.1587, -0.8878]),\n",
       " 'adversely': tensor([-0.5159, -0.7291,  0.0188,  0.0835, -1.1341]),\n",
       " 'adversity': tensor([-1.0470,  0.4255, -1.8692,  1.5001,  0.2661]),\n",
       " 'advertising': tensor([-0.0071, -0.8232,  1.3438, -0.4097,  0.0918]),\n",
       " 'advice': tensor([-0.2806,  0.7731, -1.1601, -0.6478,  2.0485]),\n",
       " 'advised': tensor([ 0.2392, -2.1594, -1.4323, -1.0143, -1.3982]),\n",
       " 'adviser': tensor([ 8.2626e-01,  8.8113e-01,  1.2415e-03, -1.1744e+00,  1.3014e+00]),\n",
       " 'advisor': tensor([-1.0278, -0.8999,  0.5017, -0.4281,  0.1424]),\n",
       " 'advisors': tensor([-0.2276,  1.5323,  0.4777, -0.7632, -1.1493]),\n",
       " 'advisory': tensor([-1.0264, -0.3655,  0.4094, -0.9170, -1.8542]),\n",
       " 'advocacy': tensor([-1.4502,  0.5451, -0.9888, -1.1508, -0.0605]),\n",
       " 'advocate': tensor([0.6537, 0.6128, 1.1136, 0.0053, 1.1022]),\n",
       " 'advocated': tensor([ 0.4181,  1.3836, -1.0091,  0.0821,  0.2154]),\n",
       " 'advocates': tensor([-1.5056,  0.3573,  0.1491,  0.0592, -1.1761]),\n",
       " 'advocating': tensor([ 0.0628,  1.1246, -1.4737, -0.2025,  0.3222]),\n",
       " 'adyghe': tensor([-0.7311, -0.2117,  0.1889,  0.9579, -1.1480]),\n",
       " 'adélie': tensor([ 0.4504, -0.5185,  0.3249, -0.7885, -0.1655]),\n",
       " 'aegean': tensor([-0.8992, -0.1054, -1.3500, -0.7371, -0.8809]),\n",
       " 'aeolian': tensor([-0.0888,  0.1852, -2.3980,  0.8138, -0.1350]),\n",
       " 'aeon': tensor([-0.0445,  0.2091,  0.8912,  1.9371, -0.4488]),\n",
       " 'aerial': tensor([ 1.1533, -0.6842,  1.2845,  0.9635,  1.2973]),\n",
       " 'aeronautica': tensor([-0.3732,  0.2619, -0.6825, -0.0571,  0.1385]),\n",
       " 'aeronautics': tensor([-0.4029,  0.2900,  0.7830,  0.4157, -1.0565]),\n",
       " 'aeronomy': tensor([-0.2860,  0.9570, -0.4345,  0.2685, -0.1536]),\n",
       " 'aeroplanes': tensor([-0.4679, -0.5847,  1.9886,  0.2943,  0.3940]),\n",
       " 'aerospace': tensor([-0.2578, -0.5598,  0.3143, -1.1561, -0.8240]),\n",
       " 'aesculapian': tensor([ 0.8449, -0.9152,  0.5407,  1.5035, -0.3906]),\n",
       " 'aesthetic': tensor([-0.8656,  1.0358,  0.9897, -0.2481, -0.8730]),\n",
       " 'afanasy': tensor([-1.0225, -1.2368, -0.6063,  2.0342,  0.4541]),\n",
       " 'afar': tensor([-1.1347,  0.1730, -0.2899, -0.0243, -0.3005]),\n",
       " 'afc': tensor([ 0.2545, -1.0245,  0.6743, -0.3294, -0.3707]),\n",
       " 'affair': tensor([ 1.8296, -0.8813,  0.9913,  1.6626, -1.3976]),\n",
       " 'affairs': tensor([ 0.3083,  0.2235,  1.2819, -0.2498, -0.4439]),\n",
       " 'affect': tensor([ 0.8602, -0.3044, -1.1864,  0.6720, -0.3127]),\n",
       " 'affected': tensor([ 1.9203, -0.3198,  0.5293, -0.8281,  1.0168]),\n",
       " 'affecting': tensor([ 0.0317, -0.6744, -0.6572, -0.0254, -0.9739]),\n",
       " 'affections': tensor([ 0.3084, -0.0606,  0.8019,  1.5090, -0.7878]),\n",
       " 'affiliated': tensor([-0.0347, -0.1764, -0.3848, -0.6271,  0.0558]),\n",
       " 'affiliates': tensor([-0.1545, -1.1805,  2.7460, -0.9881,  0.6758]),\n",
       " 'affiliation': tensor([ 0.0969,  0.0167,  1.0475, -0.4238, -0.1886]),\n",
       " 'affinity': tensor([-0.7639,  1.2192,  0.1013, -1.4861,  1.6287]),\n",
       " 'affirm': tensor([ 1.8589, -0.0517, -1.1964,  1.5357, -1.5942]),\n",
       " 'affirmative': tensor([ 0.8441, -0.4218,  1.4383,  0.2704, -1.5821]),\n",
       " 'affirmed': tensor([-1.6339, -0.0190, -0.2707, -1.0729,  0.8398]),\n",
       " 'affirming': tensor([-0.7087,  1.1469, -0.3940,  0.7610, -0.2785]),\n",
       " 'affirms': tensor([-0.0127,  0.3151,  1.1800, -1.2764,  1.0917]),\n",
       " 'afford': tensor([-0.9238,  1.1423,  0.6094, -0.1313, -0.8548]),\n",
       " 'affordability': tensor([-0.3126,  0.3196, -1.0286, -1.6322, -0.3731]),\n",
       " 'affordable': tensor([-0.6024, -0.3802, -0.3706, -0.4821,  0.7353]),\n",
       " 'afforded': tensor([ 1.2343, -1.1924, -0.1119,  0.2203, -0.2452]),\n",
       " 'affording': tensor([-0.9778, -0.9798,  1.6794,  0.0467,  1.8799]),\n",
       " 'afforestation': tensor([ 1.0488, -0.2024,  1.0342, -0.9097, -0.2513]),\n",
       " 'afghan': tensor([ 1.7563, -0.6917, -1.4366, -0.0424,  0.7215]),\n",
       " 'afghania': tensor([ 0.8064,  0.3284, -0.1807, -0.7054, -0.4283]),\n",
       " 'afghanistan': tensor([-0.8797, -0.4060, -0.7642, -0.5687, -0.1734]),\n",
       " 'afghans': tensor([-0.3105,  0.2636, -0.8885, -0.2479, -1.8634]),\n",
       " 'afn': tensor([-1.1093, -0.5391, -1.1058, -0.8547, -1.2476]),\n",
       " 'afonso': tensor([-0.3026, -1.4213,  2.3503, -2.2538,  2.1970]),\n",
       " 'afontova': tensor([ 1.0400, -0.1833, -0.6506, -0.3626, -1.9197]),\n",
       " 'aforementioned': tensor([-1.3085,  0.7079, -0.3015,  1.3921,  0.8127]),\n",
       " 'afoxê': tensor([ 0.3692, -0.9750, -0.0306, -0.1406, -0.1090]),\n",
       " 'africa': tensor([-0.0777, -1.1344, -0.9943, -0.7755, -0.6475]),\n",
       " 'african': tensor([-0.6933, -0.2855, -0.1197, -0.6383,  0.0187]),\n",
       " 'africans': tensor([-0.2886, -0.2603, -0.9499, -0.3586, -0.5605]),\n",
       " 'afrika': tensor([-0.2366,  0.5850,  0.4313, -0.9251, -2.5863]),\n",
       " 'afrin': tensor([-2.5625, -1.1127,  0.3598, -0.3102,  0.4220]),\n",
       " 'afro': tensor([ 0.4775, -0.3751,  1.3507, -0.6549, -0.0643]),\n",
       " 'afroasiatic': tensor([ 0.7723,  0.4901, -0.5665, -0.7640, -1.2145]),\n",
       " 'afrobeat': tensor([-0.9068,  0.5136, -1.7215,  0.1577,  0.3579]),\n",
       " 'afrobeats': tensor([-1.2347, -0.6985,  1.7340,  1.9433, -0.2578]),\n",
       " 'after': tensor([-0.0309,  0.6017, -1.4423, -0.1006, -0.0598]),\n",
       " 'aftermath': tensor([-1.6629,  1.5402,  0.1001, -0.4971, -1.4640]),\n",
       " 'afternoon': tensor([-0.8322, -0.7526,  0.4423,  0.3588,  0.2342]),\n",
       " 'afterward': tensor([ 0.4185,  0.1969, -1.0617, -1.2807,  0.7154]),\n",
       " 'afterwards': tensor([-0.6001,  0.0264, -1.7408, -0.9668,  0.2363]),\n",
       " 'again': tensor([ 0.0063, -0.4520,  2.2746, -0.9119,  0.5105]),\n",
       " 'against': tensor([ 0.5316, -0.9591, -0.1156,  0.5157, -0.1142]),\n",
       " 'agal': tensor([ 0.0128, -0.3882,  0.5495, -0.0579,  2.1971]),\n",
       " 'agama': tensor([0.6263, 0.6676, 0.8942, 0.2636, 0.6575]),\n",
       " 'agate': tensor([-0.2902, -0.9299,  0.8156, -0.1714,  0.1706]),\n",
       " 'agatha': tensor([-0.9421, -0.1103, -0.1674,  0.4182,  0.7641]),\n",
       " 'agave': tensor([ 1.6556,  0.8680, -0.5507, -1.2001,  0.1195]),\n",
       " 'age': tensor([ 0.0388, -0.5377,  0.5011, -0.1167, -0.6697]),\n",
       " 'aged': tensor([ 1.4649,  0.1508,  0.5278, -0.3275, -0.1509]),\n",
       " 'agencies': tensor([ 0.4773, -0.4229,  0.0607, -0.9638,  0.5906]),\n",
       " 'agency': tensor([ 1.5604, -0.2758,  0.1189, -0.1131,  0.8793]),\n",
       " 'agenda': tensor([-0.1279,  0.6338, -1.4816,  1.4136,  0.0472]),\n",
       " 'agents': tensor([ 0.6822, -1.0034, -0.4438, -0.4125,  0.9286]),\n",
       " 'ages': tensor([-0.4216,  0.9311, -0.4182, -0.4500,  0.2983]),\n",
       " 'agglomerations': tensor([ 1.8423, -0.2296,  0.9172, -1.1222, -0.8857]),\n",
       " 'aggravated': tensor([-0.1418, -0.6962,  1.0788, -0.0040, -0.6040]),\n",
       " 'aggregate': tensor([ 0.7962, -0.1055, -0.2732,  0.1039,  0.8851]),\n",
       " 'aggregated': tensor([ 1.6228,  0.5707, -0.6222,  0.8677,  0.3496]),\n",
       " 'aggression': tensor([-0.0304, -0.2250, -0.5005,  0.0184, -0.6901]),\n",
       " 'aggressive': tensor([ 0.4051, -0.6526, -0.2150,  0.4007,  0.4790]),\n",
       " 'agha': tensor([ 0.5357, -0.3417, -1.4907,  0.2990,  0.2525]),\n",
       " 'aging': tensor([ 0.4823,  0.3537, -1.0650, -0.8756,  0.1901]),\n",
       " 'agitating': tensor([-1.4973, -0.4942,  0.1370, -1.4558,  1.3271]),\n",
       " 'agitations': tensor([ 1.3373,  1.1311, -0.3369, -0.8278,  0.3501]),\n",
       " 'agnosticism': tensor([-1.6907,  0.4249,  0.6545, -1.9819,  0.5337]),\n",
       " 'agnostics': tensor([-0.2056,  0.1173,  0.0193,  0.5316,  0.3857]),\n",
       " 'ago': tensor([ 0.4252,  1.3856,  1.5614, -0.4340,  0.1240]),\n",
       " 'agra': tensor([ 0.8905, -0.6593, -0.5557,  0.4983,  0.2569]),\n",
       " 'agrarian': tensor([ 0.5704,  0.3162, -0.7450,  0.6685,  1.2433]),\n",
       " 'agree': tensor([ 1.2358, -0.9128,  1.2887, -0.5618, -0.8884]),\n",
       " 'agreeable': tensor([-0.5947,  0.0658, -0.6339, -0.0026, -1.2472]),\n",
       " 'agreed': tensor([-1.3375,  0.1111, -0.3193, -0.1545, -0.1621]),\n",
       " 'agreeing': tensor([-0.2771,  0.2555, -1.3798, -0.5622,  0.7824]),\n",
       " 'agreement': tensor([ 0.7502, -0.5203, -0.4212, -0.0801, -0.3173]),\n",
       " 'agreements': tensor([ 0.5541, -0.9114,  0.3173, -0.9498, -0.6302]),\n",
       " 'agricultural': tensor([ 0.3212,  0.1845, -0.3988, -0.4726, -0.1242]),\n",
       " 'agriculture': tensor([ 1.0784,  0.0252,  0.7311, -0.3122, -0.1631]),\n",
       " 'agritourism': tensor([ 0.8517, -1.1896,  0.1811, -0.7063, -1.1321]),\n",
       " 'agua': tensor([ 0.3641, -0.1966,  0.5717,  1.3280, -0.1949]),\n",
       " 'aguas': tensor([-1.5200, -1.2917,  0.0659,  0.3800,  0.4482]),\n",
       " 'aguleri': tensor([-1.1922, -1.0941, -0.7623,  0.4517, -0.6203]),\n",
       " 'agung': tensor([ 1.5684, -0.2197,  0.3338,  0.3920,  1.5285]),\n",
       " 'agustín': tensor([-1.2677, -0.5821,  0.2625, -0.8814,  1.0918]),\n",
       " 'ahead': tensor([ 0.2033, -0.6743, -0.9251, -0.1440, -1.3597]),\n",
       " 'ahimsa': tensor([ 0.0183,  0.4322, -0.2399,  0.3773,  0.8607]),\n",
       " 'ahly': tensor([-0.7283, -0.1007,  0.1428, -1.1324, -0.0108]),\n",
       " 'ahmad': tensor([-1.2678,  0.3478,  0.9767,  0.4139,  1.2965]),\n",
       " 'ahmadinejad': tensor([-0.7419, -0.9617, -0.1545, -1.0132,  1.3220]),\n",
       " 'ahmadis': tensor([ 1.5471, -0.1172,  0.2735,  0.5820,  0.1014]),\n",
       " 'ahmadiyya': tensor([ 0.2296,  0.9678,  1.1459,  0.7896, -0.5523]),\n",
       " 'ahmadiyyas': tensor([-1.5891,  1.3576,  0.1820, -1.0049,  0.1694]),\n",
       " 'ahmadu': tensor([-0.9583,  0.1522, -0.3305,  1.2160, -0.5186]),\n",
       " 'ahmed': tensor([-0.0855, -0.1531,  0.8905, -0.0680, -1.4247]),\n",
       " 'ahmedabad': tensor([-1.5919,  0.0724,  0.3034,  1.8195,  0.6269]),\n",
       " 'ahmet': tensor([-1.3835, -0.6677,  1.9577,  1.2882,  0.3417]),\n",
       " 'ahmose': tensor([0.6314, 0.5383, 0.6726, 1.8772, 0.7377]),\n",
       " 'ahriman': tensor([-0.7720, -0.6917, -1.3730,  2.1221, -0.4188]),\n",
       " 'ahura': tensor([-1.1089,  0.0599,  0.0690,  0.5809, -0.4941]),\n",
       " 'ahvaz': tensor([-0.5895, -0.0522, -0.0486, -0.6705,  0.5213]),\n",
       " 'aian': tensor([ 1.2792, -0.4191, -0.7516, -0.3629,  0.3098]),\n",
       " 'aichi': tensor([-1.2400, -0.0628, -1.2052,  0.0657,  0.2740]),\n",
       " 'aid': tensor([ 0.7764,  0.0982,  0.4282, -0.4326,  0.0404]),\n",
       " 'aide': tensor([-0.4746,  0.7695,  1.8219, -0.7548, -0.7989]),\n",
       " 'aided': tensor([ 0.2493, -0.9702, -1.2904,  0.4145,  0.2994]),\n",
       " 'aides': tensor([-1.3327, -0.9993, -1.1394, -1.4912,  0.1696]),\n",
       " 'aiding': tensor([-0.3553, -1.0118,  0.6326,  0.2286, -0.0765]),\n",
       " 'aids': tensor([-1.7432,  1.1894,  0.0035,  2.0915,  0.0076]),\n",
       " 'ailing': tensor([ 0.7708,  0.6437, -0.6722, -0.9586, -0.1320]),\n",
       " 'aim': tensor([ 1.0625,  1.8134, -0.1604, -0.0249,  0.5879]),\n",
       " 'aimed': tensor([ 0.7055, -0.6897, -1.2167, -0.1221,  0.5953]),\n",
       " 'aiming': tensor([-0.0255, -0.0477, -0.3061,  0.2195, -0.0781]),\n",
       " 'aims': tensor([ 0.1380, -0.9824,  0.2244, -0.5545, -1.4078]),\n",
       " 'ainu': tensor([ 0.1464, -0.7055,  0.4550, -0.7982, -1.7280]),\n",
       " 'air': tensor([ 1.8774, -0.1752, -0.6920, -0.1506,  1.9642]),\n",
       " 'airblue': tensor([-1.0971, -1.4360, -0.9165,  0.8640,  0.4436]),\n",
       " 'airborne': tensor([ 0.1521, -1.6892, -1.1668,  0.1049, -0.0670]),\n",
       " 'airbus': tensor([-2.3274,  0.7333, -0.3024,  0.6700, -0.2761]),\n",
       " 'aircraft': tensor([-1.6709,  0.5670, -0.6092,  0.3266,  1.4484]),\n",
       " 'aired': tensor([-0.9497, -0.4850, -1.5811, -1.3494,  0.1346]),\n",
       " 'airfields': tensor([ 0.1380,  0.5060,  0.5707, -1.6109,  0.0779]),\n",
       " 'airlifted': tensor([ 0.1733, -1.2709, -0.9409,  0.0084,  0.0726]),\n",
       " 'airline': tensor([ 0.6330,  0.7569,  0.0428, -0.2740,  0.9772]),\n",
       " 'airlines': tensor([ 1.3505,  0.4277,  0.0189, -0.2428, -0.1137]),\n",
       " 'airplane': tensor([-1.2911,  0.2051,  0.3621, -1.8885,  0.0649]),\n",
       " 'airplanes': tensor([ 0.3041,  0.6157,  2.0682,  0.3547, -0.2733]),\n",
       " 'airport': tensor([ 1.0857,  0.3364, -0.6876, -0.3006, -0.1338]),\n",
       " 'airports': tensor([ 0.0585,  0.5844, -0.1297, -0.5708,  0.8024]),\n",
       " 'airs': tensor([ 0.7033, -0.4545,  0.7614,  0.3623, -0.0421]),\n",
       " 'airspace': tensor([-0.0665, -0.0365,  0.1201, -0.1830,  1.3845]),\n",
       " 'airtel': tensor([-0.7026,  0.1133, -2.2388,  1.7427,  0.7512]),\n",
       " 'airways': tensor([ 1.6625, -0.4045, -1.1035,  0.6994, -1.7095]),\n",
       " 'aivazovsky': tensor([-0.7615,  0.0790,  0.3742,  2.8036,  1.0349]),\n",
       " 'ajanta': tensor([ 1.9811,  0.2778, -1.4751,  0.3836, -0.3907]),\n",
       " 'ajaokuta': tensor([ 0.4574,  0.8888, -1.2354,  0.0512,  0.0691]),\n",
       " 'ajayi': tensor([-1.3513, -0.8723, -0.2748, -0.9978, -0.6520]),\n",
       " 'ajda': tensor([0.4435, 0.4580, 1.1156, 0.7539, 0.2701]),\n",
       " 'ajk': tensor([-0.0612, -0.7714, -0.6788, -0.5409, -1.8335]),\n",
       " 'ajudani': tensor([ 0.3506,  1.7815,  0.8909,  0.0418, -0.9881]),\n",
       " 'akatsuki': tensor([-0.5913, -0.3563,  0.4807,  1.9616,  1.4276]),\n",
       " 'akbar': tensor([1.6172, 0.6560, 0.0370, 0.1383, 0.2648]),\n",
       " 'akbaş': tensor([ 0.7223, -0.5853, -0.7697, -0.8046, -0.1378]),\n",
       " 'akdoğan': tensor([ 0.3374, -0.4828,  0.6731, -0.6981, -1.5637]),\n",
       " 'akhenaten': tensor([-1.0038,  0.6434, -2.2466,  1.4175,  1.3570]),\n",
       " 'akhmatova': tensor([-1.4605, -0.3262, -0.1882,  1.1170,  0.3531]),\n",
       " 'akihito': tensor([ 1.9315, -0.6738,  0.7722,  0.2148, -1.8723]),\n",
       " 'akintola': tensor([ 0.7549, -0.5718,  1.4932, -0.2510, -0.0346]),\n",
       " 'akitoye': tensor([ 0.4606, -1.0565, -0.3277, -1.0748, -0.5525]),\n",
       " 'akkadian': tensor([-0.0282,  1.0056, -1.3223,  0.9471, -0.5519]),\n",
       " 'akkas': tensor([-0.0604, -1.0795, -0.6612,  0.5964,  0.5715]),\n",
       " 'akkuyu': tensor([ 1.6337, -0.1128,  0.6509, -0.6774,  2.3111]),\n",
       " 'akp': tensor([ 0.7224,  0.3842, -1.4970, -0.8161,  0.4811]),\n",
       " 'akragas': tensor([ 1.7133, -0.0091, -1.6750,  2.0080, -1.7876]),\n",
       " 'akrotiri': tensor([-0.0544, -2.2597, -1.4340, -0.4309,  0.5358]),\n",
       " 'aksai': tensor([ 1.0337, -0.5035,  0.0858, -0.1994, -0.4061]),\n",
       " 'aksiyon': tensor([ 2.0634, -0.9717,  0.2505,  0.5647, -0.7745]),\n",
       " 'aksu': tensor([-1.2583, -1.0976,  0.1880,  1.5260, -1.1459]),\n",
       " 'akutagawa': tensor([-1.6879,  1.0153,  0.6265,  1.6849,  1.2690]),\n",
       " 'akwa': tensor([-0.1357,  1.2004, -0.2594,  0.1756,  0.5530]),\n",
       " 'akşehir': tensor([0.6177, 0.2290, 2.4964, 0.5000, 0.3289]),\n",
       " 'al': tensor([-0.2663, -0.4167, -0.0842, -0.1650, -0.2624]),\n",
       " 'alagoas': tensor([-0.2141, -0.5085,  0.1848, -0.3691, -1.2111]),\n",
       " 'alaide': tensor([-2.1592,  0.3115,  0.0884,  1.0394,  1.2174]),\n",
       " 'alam': tensor([ 0.3865,  0.7032, -1.4994, -1.3068, -0.4351]),\n",
       " 'alamgiri': tensor([-0.2399, -0.4402, -0.6136,  0.3662,  0.2365]),\n",
       " 'alarcón': tensor([-1.9085, -0.7638, -1.5461, -0.8071, -0.3360]),\n",
       " 'alardah': tensor([-0.1189,  0.3942,  0.3322,  1.8023,  0.9750]),\n",
       " 'alarm': tensor([-0.4024,  1.2214,  0.3943, -0.6391, -0.9435]),\n",
       " 'alaska': tensor([-0.7835,  0.6267,  0.1216, -0.5895,  0.3923]),\n",
       " 'alassane': tensor([-1.0984, -0.5028,  0.5147,  0.1579,  0.0786]),\n",
       " 'albanese': tensor([ 1.4849, -2.0816, -2.3644, -0.9580,  1.1180]),\n",
       " 'albania': tensor([ 0.4702, -1.4086, -0.8259, -0.2385,  0.8767]),\n",
       " 'albanian': tensor([-0.4079,  0.2360,  0.3241, -0.7638,  0.8440]),\n",
       " 'albanians': tensor([ 1.1635,  2.3629,  2.2865, -1.4630, -0.1456]),\n",
       " 'albany': tensor([ 0.2798,  0.9292, -1.1648, -0.5248, -1.6025]),\n",
       " 'albeit': tensor([ 0.7721,  0.1861,  1.6144, -0.1638,  0.9965]),\n",
       " 'albernaz': tensor([-0.5675, -0.9183, -0.3064,  0.1314,  0.8626]),\n",
       " 'albert': tensor([-1.6659, -1.0616,  0.6652,  2.2731,  0.4980]),\n",
       " 'alberta': tensor([ 0.6200,  0.5251, -1.9441,  1.0107, -0.8973]),\n",
       " 'alberti': tensor([ 0.5944, -1.4112, -0.3645,  0.0609,  0.7191]),\n",
       " 'albertine': tensor([ 1.1666, -0.1766, -1.5098, -0.4402,  0.5851]),\n",
       " 'alberto': tensor([-1.1040, -0.8591,  0.2104,  1.4940, -0.3132]),\n",
       " 'albertville': tensor([-0.1854, -1.6578, -0.0285, -1.6651, -1.4818]),\n",
       " 'albigensian': tensor([-1.0547, -0.9168, -0.7284, -0.3602, -0.3592]),\n",
       " 'alborz': tensor([-1.5944, -0.4784,  2.0628,  0.6763, -2.1311]),\n",
       " 'albrecht': tensor([-1.3654, -0.4107,  1.3516,  0.1237, -0.0667]),\n",
       " 'albuquerque': tensor([ 0.6459,  0.8506, -0.5750,  1.4103, -1.1096]),\n",
       " 'albán': tensor([ 0.7590, -0.1794, -0.4658, -0.4287,  0.5580]),\n",
       " 'alcide': tensor([-0.0744,  0.0485,  0.8490,  1.6864,  0.7162]),\n",
       " 'alcohol': tensor([-1.2386,  0.3861, -1.2733, -0.1247, -0.7160]),\n",
       " 'alcoholic': tensor([-1.6230,  0.6785, -0.5501, -0.3215, -0.4029]),\n",
       " 'alcântara': tensor([ 0.1824,  0.2558, -2.3473, -0.7418,  2.0377]),\n",
       " 'aldo': tensor([-1.5220,  0.0121,  1.1686,  1.6136,  0.0641]),\n",
       " 'alegre': tensor([-0.8504,  0.4237, -0.7125, -0.1247,  0.1776]),\n",
       " 'alejandro': tensor([-0.3123,  1.9923,  0.2635,  1.3894, -0.5138]),\n",
       " 'aleksandr': tensor([ 0.5963,  1.7511, -0.3271,  1.1587, -1.0062]),\n",
       " 'aleksey': tensor([-0.9267,  0.3287, -0.1639,  1.0502, -1.2323]),\n",
       " 'alemannic': tensor([ 0.8406,  0.4919,  0.6472, -0.0623, -0.1476]),\n",
       " 'alemán': tensor([ 0.0845,  0.1866, -0.5418, -0.7863, -0.4535]),\n",
       " 'alencar': tensor([-0.4058, -0.9967, -0.6026,  0.0344,  0.8838]),\n",
       " 'aleramo': tensor([-0.4819,  1.8170,  0.7275, -0.1862, -0.3798]),\n",
       " 'alert': tensor([-0.2921, -0.8579, -0.5202,  0.4891, -0.7029]),\n",
       " 'alessandro': tensor([0.1503, 0.4239, 0.3454, 1.5990, 0.2830]),\n",
       " 'alestra': tensor([-0.8516,  0.5271, -0.7107,  1.9074, -0.8200]),\n",
       " 'aleutian': tensor([ 0.6510, -0.9724, -0.4313,  1.0517,  0.3816]),\n",
       " 'alevi': tensor([-0.0687,  0.1411, -0.3466,  0.4850, -1.3563]),\n",
       " 'alevis': tensor([-0.7892, -0.6160, -1.3881, -0.0240, -1.8470]),\n",
       " 'alevites': tensor([ 0.2134, -0.5636,  0.9096, -1.4237,  0.5990]),\n",
       " 'alexa': tensor([ 0.5059, -0.0154,  1.4341,  0.2053,  0.7031]),\n",
       " 'alexander': tensor([-0.4396, -0.1935,  0.5358, -0.2610, -0.2656]),\n",
       " 'alexandre': tensor([-1.4839,  0.0940, -0.8375, -0.2444,  1.1848]),\n",
       " 'alexandria': tensor([-1.2284,  0.1329, -0.0527, -1.4912,  0.5400]),\n",
       " 'alexandropol': tensor([ 0.5562, -1.6954, -0.5046, -0.2056,  0.2551]),\n",
       " 'alexei': tensor([-0.1042, -0.9292,  0.5312, -0.8332,  0.4708]),\n",
       " 'alexeieff': tensor([-0.2751, -0.0320, -1.0045,  0.6051, -0.5289]),\n",
       " 'alexis': tensor([-1.0387,  0.8842, -1.1871, -1.4971, -0.3952]),\n",
       " 'alfa': tensor([-0.0972,  1.4834,  0.0396,  1.7786,  1.0809]),\n",
       " 'alfaro': tensor([-0.1180, -0.8435, -1.1643,  2.5578,  1.7587]),\n",
       " 'alfieri': tensor([-1.2694, -0.0793,  0.0969,  1.7474,  0.2030]),\n",
       " 'alfonso': tensor([-2.6929,  1.4582,  0.1288,  1.9958,  0.6692]),\n",
       " 'alfred': tensor([ 1.1813,  0.1666, -0.1523,  0.2466, -0.4354]),\n",
       " 'algae': tensor([ 0.9989,  1.3436, -1.8330, -1.2523, -0.6120]),\n",
       " 'algal': tensor([-1.8443, -1.4018,  0.6428, -0.1839, -1.3005]),\n",
       " 'algarves': tensor([ 0.0459, -1.5048,  1.1883, -0.4106,  1.0124]),\n",
       " 'algeria': tensor([-0.5675, -0.8975,  0.2873, -0.3049, -0.7686]),\n",
       " 'algerian': tensor([-0.4026,  1.8293, -0.0830, -0.7182,  1.0008]),\n",
       " 'algerians': tensor([-0.3501,  1.0747, -0.7159, -0.2640,  1.1470]),\n",
       " 'algiers': tensor([ 1.4909,  0.9534, -0.1907, -0.8026, -0.6182]),\n",
       " 'algonquian': tensor([ 0.3556, -0.9183,  1.4117, -0.1067, -0.1469]),\n",
       " 'algonquin': tensor([-1.4043, -1.8813,  0.6689, -3.1415,  0.5402]),\n",
       " 'algosaibi': tensor([-1.9489, -0.7250, -0.6981, -0.9946,  0.1960]),\n",
       " 'alhaji': tensor([-1.0089, -0.1436, -1.0332,  0.5084,  0.6598]),\n",
       " 'ali': tensor([ 0.2617, -0.5166, -1.2901,  0.3817,  1.2576]),\n",
       " 'alice': tensor([-0.6327,  0.2103, -0.5110,  0.1863,  2.1150]),\n",
       " 'alien': tensor([ 0.2892, -0.9024,  0.5084, -0.6941,  1.3299]),\n",
       " 'alienated': tensor([-1.8068,  0.8344, -0.4715,  1.5304,  0.7213]),\n",
       " 'alienating': tensor([ 1.2280, -0.4322, -0.3947,  1.7329, -0.4179]),\n",
       " 'alienation': tensor([ 1.0359, -0.6934,  0.7720, -0.9643, -1.3747]),\n",
       " 'alifa': tensor([-0.5855, -0.9276, -1.5621, -0.8513,  0.3947]),\n",
       " 'alighieri': tensor([ 1.3694, -1.2539, -0.9309, -0.2087,  0.4706]),\n",
       " 'aligned': tensor([ 0.3099,  1.1469,  1.0854, -0.8396,  0.2817]),\n",
       " 'alignment': tensor([-0.6334,  0.2512, -1.4781,  0.2459, -0.9986]),\n",
       " 'alignments': tensor([-1.9450,  0.8076,  0.7770,  0.3665,  0.9732]),\n",
       " 'aliko': tensor([ 0.8427,  0.8058,  0.6801, -0.2933,  0.4048]),\n",
       " 'alim': tensor([-1.1182, -0.0327,  0.4487, -1.8075,  0.2617]),\n",
       " 'alitalia': tensor([-1.4601, -0.4341,  0.6280, -0.3100,  1.8325]),\n",
       " 'alive': tensor([ 0.3111,  1.9273, -1.1781, -1.8404, -0.6362]),\n",
       " 'alkali': tensor([-0.6722,  1.1157,  1.2960, -0.9507,  0.5198]),\n",
       " 'alkaline': tensor([-1.1917,  0.0577,  1.6899, -2.4924, -0.0135]),\n",
       " 'all': tensor([-1.0006,  0.1433, -0.9388, -0.6031, -0.1998]),\n",
       " 'alla': tensor([-0.3091,  0.1263,  0.4263,  0.1434, -0.8328]),\n",
       " 'allah': tensor([ 0.5741, -0.1285,  0.9845, -0.0667, -0.9269]),\n",
       " 'allahu': tensor([ 0.2871,  0.5796, -0.0406,  3.1601, -0.3224]),\n",
       " 'allama': tensor([-0.5465,  1.0210,  0.2221, -0.3551,  0.5737]),\n",
       " 'allay': tensor([-1.3596, -0.7420,  0.1271, -0.9950, -0.5979]),\n",
       " 'allegations': tensor([ 0.4745, -1.6959, -0.7132,  1.0990,  0.6167]),\n",
       " 'alleged': tensor([ 0.8043,  0.1696, -1.5426, -0.9310,  0.7261]),\n",
       " 'allegedly': tensor([ 0.0037, -0.2594, -2.0757, -0.0704,  0.4590]),\n",
       " 'allegiance': tensor([-1.1175,  1.3540,  0.1202, -0.5497,  1.3526]),\n",
       " 'alleging': tensor([ 0.1895,  0.4567, -0.4857, -0.8176, -0.3401]),\n",
       " 'alleviation': tensor([ 0.2541,  1.4335, -0.8957,  0.6503,  0.0755]),\n",
       " 'alley': tensor([ 0.2723, -1.0303,  0.0689, -0.6663, -2.0109]),\n",
       " 'allgemeine': tensor([-2.1816,  1.1258, -0.0868,  2.0996, -0.1096]),\n",
       " 'allia': tensor([-1.1479, -0.1259,  0.2163,  0.1345,  0.3468]),\n",
       " 'alliance': tensor([-0.4309,  0.2997,  0.3903, -0.2194, -0.2950]),\n",
       " 'alliances': tensor([-0.9954,  0.3466,  0.0847, -0.7511, -0.0431]),\n",
       " 'allianz': tensor([-0.3940, -0.9504, -0.5482,  0.5588, -1.4026]),\n",
       " 'allied': tensor([-1.1545,  0.3116, -1.0643, -0.8241,  0.6013]),\n",
       " 'allies': tensor([ 0.1792,  0.6651, -0.9378, -0.2893, -0.3675]),\n",
       " 'allocate': tensor([ 0.3202,  1.1166,  0.9097, -0.7647, -0.2822]),\n",
       " 'allocated': tensor([-0.4377,  1.4140, -2.1185,  0.0187, -1.1408]),\n",
       " 'allocating': tensor([ 1.2716,  1.1043,  0.4032, -0.9935, -1.3531]),\n",
       " 'allocations': tensor([-0.1074, -0.5088,  0.6667, -0.9062, -0.3495]),\n",
       " 'allow': tensor([ 0.3511,  0.5624, -0.4830,  0.1756, -0.3923]),\n",
       " 'allowances': tensor([ 1.8698,  0.7685,  0.0756, -0.3147, -0.0552]),\n",
       " 'allowed': tensor([ 0.1073,  0.3773,  0.1763, -0.3907,  0.2482]),\n",
       " 'allowing': tensor([ 0.7791, -0.5551,  0.8947, -0.5572,  0.2147]),\n",
       " 'allows': tensor([-0.7926, -0.4425, -0.9368, -0.5032,  0.5868]),\n",
       " 'allusion': tensor([-5.6305e-01, -6.5367e-05,  5.1549e-01,  8.8329e-01,  4.9202e-01]),\n",
       " 'alluvial': tensor([-0.9672,  0.5141,  0.2654, -0.2333,  0.5149]),\n",
       " 'ally': tensor([-0.2542, -0.1917,  0.1557, -0.4571, -0.6078]),\n",
       " 'almarai': tensor([ 0.7119, -1.3969, -0.0962,  0.3134,  0.0711]),\n",
       " 'almaz': tensor([-0.7989, -1.1063,  0.8658,  0.0228, -0.2192]),\n",
       " 'almeida': tensor([-0.7345, -1.0730,  0.0146, -0.0851,  0.8787]),\n",
       " 'almezmar': tensor([-3.0024, -0.4772,  0.5103,  0.8455, -0.9638]),\n",
       " 'almighty': tensor([-0.2555,  0.1332, -1.5967,  0.4852, -0.2145]),\n",
       " 'almonds': tensor([-1.2327, -0.0217, -0.6314,  0.4133,  0.2376]),\n",
       " 'almost': tensor([ 0.3957, -0.1590, -0.0738, -0.9223, -0.2537]),\n",
       " 'alnajdiyah': tensor([ 0.8075, -0.5559, -0.1200,  0.1800, -1.0516]),\n",
       " 'alone': tensor([-0.3514,  0.1593, -0.1984, -0.4333, -0.9689]),\n",
       " 'along': tensor([-0.9306,  0.1365, -0.2630,  0.0420, -0.4698]),\n",
       " 'alongside': tensor([-0.7575, -0.6740, -0.5583, -0.4373, -0.1394]),\n",
       " 'alonso': tensor([-0.5876, -0.0588,  0.1711, -0.5328,  0.3960]),\n",
       " 'alouette': tensor([-0.3324,  0.0344,  0.3355,  0.6396,  0.3082]),\n",
       " 'alpha': tensor([-0.4825,  1.2987,  0.1582, -0.8239, -0.7947]),\n",
       " 'alphabet': tensor([ 0.0332,  0.1969, -0.2206, -0.9182, -0.1211]),\n",
       " 'alphabetic': tensor([ 0.3853, -1.1901, -2.3060,  0.5104, -1.6184]),\n",
       " 'alphabetically': tensor([-1.2465,  0.2422, -1.9321, -0.5367, -0.8918]),\n",
       " 'alpine': tensor([ 0.1073, -0.1445,  0.2450,  0.3611, -0.4555]),\n",
       " 'alps': tensor([-1.7928, -0.3068, -1.6280, -0.1007, -0.0752]),\n",
       " 'alrabiah': tensor([-0.4546,  0.9415, -0.5359, -0.2693,  1.4888]),\n",
       " 'already': tensor([-0.3835,  0.0226,  0.0369, -0.0271, -0.4034]),\n",
       " 'alsace': tensor([ 1.1913, -0.4387,  0.2155,  1.5398,  1.1150]),\n",
       " 'alsatian': tensor([-0.9917,  0.2130,  2.0110,  1.3715, -2.1332]),\n",
       " 'also': tensor([ 0.4428,  0.0975,  0.1501, -0.1019, -0.2735]),\n",
       " 'alta': tensor([ 0.5571, -0.6795,  0.1915, -0.0582,  0.0837]),\n",
       " 'altai': tensor([ 0.6398,  0.5592, -0.6131,  0.4887, -0.2207]),\n",
       " 'altaic': tensor([ 0.9564,  0.1006,  0.6296,  1.3667, -0.1694]),\n",
       " 'altamirano': tensor([ 0.2897,  1.4387,  0.8159, -1.0517,  2.7922]),\n",
       " 'altamura': tensor([-1.7640, -0.5807, -1.0522,  1.3296,  0.0066]),\n",
       " 'altare': tensor([-1.2933,  0.2698,  0.7707,  0.3243, -1.0734]),\n",
       " 'alteration': tensor([0.3419, 0.4796, 0.0914, 1.3288, 0.1582]),\n",
       " 'altered': tensor([-0.8881, -0.1576,  0.0995, -0.3333, -0.3759]),\n",
       " 'alternate': tensor([ 0.7653, -0.2282,  2.0585,  1.6456,  0.9280]),\n",
       " 'alternated': tensor([ 0.7154, -0.9978, -0.5663, -0.3652, -0.0367]),\n",
       " 'alternately': tensor([-1.2836,  1.1472, -1.2075,  0.6317,  0.2831]),\n",
       " 'alternating': tensor([ 0.6614, -1.6933,  0.3289,  0.3534,  0.3101]),\n",
       " 'alternative': tensor([ 0.2807, -0.6160,  0.5786,  0.2706, -1.2337]),\n",
       " 'alternatively': tensor([-0.8369,  0.4263,  1.1436,  0.5189,  1.6167]),\n",
       " 'alternatives': tensor([-0.0356,  0.1280,  0.0811, -0.2618,  0.2720]),\n",
       " 'although': tensor([-0.7076,  0.2578,  0.3814,  0.0924,  0.1690]),\n",
       " 'altitude': tensor([ 0.0632, -0.2352,  0.0034, -0.8782, -1.6765]),\n",
       " 'altitudes': tensor([-0.3762,  1.9470, -0.9898, -0.2755, -0.6575]),\n",
       " 'alto': tensor([-0.6307, -0.3115,  0.1093,  0.8299,  0.2908]),\n",
       " 'altogether': tensor([ 0.3096, -0.5093, -0.6879,  0.6203, -1.1243]),\n",
       " 'altruism': tensor([-0.1038,  0.5232,  1.1808,  1.1783, -0.0067]),\n",
       " 'aluminum': tensor([-0.7355, -1.0329, -0.7495,  0.9719, -1.0476]),\n",
       " 'alumnus': tensor([-0.5776,  1.0046,  0.2569, -0.2790,  0.4308]),\n",
       " 'aluwaisheg': tensor([-2.2475, -0.4107, -0.8502, -1.6834, -0.9098]),\n",
       " 'alvaro': tensor([2.0518, 1.1316, 0.0218, 1.3634, 1.9317]),\n",
       " 'alvi': tensor([-0.2538, -1.3078,  0.5006, -0.1106,  0.7248]),\n",
       " 'always': tensor([ 0.2953,  0.2885,  0.5169, -0.2746, -0.6123]),\n",
       " 'aly': tensor([-0.5197, -0.1665, -0.9013, -0.7202,  0.1650]),\n",
       " 'alzheimer': tensor([ 2.4047, -1.5147, -0.1171, -0.5652, -0.7672]),\n",
       " 'amado': tensor([ 0.6758, -0.7142,  0.6864,  0.7919, -1.3798]),\n",
       " 'amalfi': tensor([ 0.3201,  2.3508, -0.8264,  1.9071, -1.3491]),\n",
       " 'amalgam': tensor([ 0.8860, -0.6596,  0.4615,  0.8848,  1.1094]),\n",
       " 'amalgamating': tensor([-0.2163,  0.7934,  0.8896, -0.3219, -0.3355]),\n",
       " 'amalgamation': tensor([-0.8395,  0.4078,  0.4677, -0.4813,  0.7699]),\n",
       " 'amami': tensor([ 0.7583,  1.9663, -0.3041,  1.1323, -0.0826]),\n",
       " 'amanah': tensor([-0.4439, -0.5453, -0.5583,  1.4989, -0.6960]),\n",
       " 'amapá': tensor([ 0.4934,  0.2031,  1.1460, -0.7060, -0.8490]),\n",
       " 'amaral': tensor([-1.3489,  0.4014, -0.2611,  0.8607,  0.2030]),\n",
       " 'amaravati': tensor([-0.6643,  0.3589, -0.1782, -0.1875,  0.3459]),\n",
       " 'amarela': tensor([ 0.1618, -0.4886,  1.6619,  0.8536, -0.9156]),\n",
       " 'amaro': tensor([ 0.6873, -0.5702,  1.4293,  0.2231, -0.7790]),\n",
       " 'amassing': tensor([ 0.2231,  0.5671, -0.0810,  1.2215,  1.8569]),\n",
       " 'amaterasu': tensor([-0.7708,  1.0541, -0.5767,  0.4158, -0.5715]),\n",
       " 'amateur': tensor([-0.2294,  0.5941,  0.9038,  0.5947, -0.0566]),\n",
       " 'amazigh': tensor([-0.6970, -0.4758,  0.0981,  0.0798,  0.1620]),\n",
       " 'amazon': tensor([ 0.3102, -0.6260,  0.7198, -0.1689, -0.6364]),\n",
       " 'amazonas': tensor([-1.1354,  1.5624, -0.1767,  0.2740,  0.8412]),\n",
       " 'ambassador': tensor([ 0.7061,  1.3929, -0.7512, -0.1104, -0.2656]),\n",
       " 'ambassadors': tensor([ 1.1194,  1.1931, -0.6084, -0.2367, -1.0519]),\n",
       " 'ambazonia': tensor([-0.6155,  1.3326, -0.2655, -0.2582, -0.7167]),\n",
       " 'ambient': tensor([-1.3002,  0.4064, -0.7555, -0.5347,  0.3145]),\n",
       " 'ambition': tensor([-0.4849, -0.0011,  0.8423, -0.8781,  0.8628]),\n",
       " 'ambitions': tensor([ 0.4227, -0.3343, -0.0969, -0.3552, -0.3013]),\n",
       " 'ambitious': tensor([ 0.6496,  1.0433, -0.3109, -0.6896, -0.4538]),\n",
       " 'ambrose': tensor([-0.4546, -1.4480,  0.8708, -1.3217,  0.7509]),\n",
       " 'amedeo': tensor([-2.2595,  0.2487, -0.0254,  1.4464, -1.7968]),\n",
       " 'amenable': tensor([-1.2368,  0.1363, -2.1535,  1.5847, -0.5396]),\n",
       " 'amended': tensor([ 0.5823, -1.2854, -0.1084, -0.3665,  0.2907]),\n",
       " 'amending': tensor([-1.0896,  0.7379,  1.3948,  0.5411, -1.7666]),\n",
       " 'amendment': tensor([ 0.4017, -0.7238, -0.3247, -0.9648,  0.5968]),\n",
       " 'amendments': tensor([ 1.7408, -1.3757,  0.4881,  1.3050, -1.0682]),\n",
       " 'amenemhat': tensor([-0.0937, -0.2582,  1.0520, -0.0606, -0.4086]),\n",
       " 'amenities': tensor([ 0.2601, -0.8623,  0.0898,  0.6931, -0.2248]),\n",
       " 'america': tensor([-0.2449, -0.1698, -1.2841, -0.4227,  0.4365]),\n",
       " 'american': tensor([-0.0084,  0.1863, -0.6028, -0.4929,  0.2810]),\n",
       " 'americans': tensor([ 0.0812, -0.0488, -0.3679,  0.6564, -0.7137]),\n",
       " 'americas': tensor([ 0.6901,  0.6129,  0.6809, -0.4798,  0.2625]),\n",
       " 'amerigo': tensor([-0.9372,  0.3435, -1.1784,  0.4974,  0.0248]),\n",
       " 'amerindian': tensor([ 0.2295, -0.6963, -0.1689,  0.0492,  0.6141]),\n",
       " 'amerindians': tensor([-0.2769, -0.7712,  1.3115, -0.8093, -0.8224]),\n",
       " 'amethyst': tensor([ 1.3295, -1.4531, -1.3406, -0.4051, -0.3395]),\n",
       " 'amharic': tensor([ 1.4879,  0.4341,  2.0364, -0.2176, -0.7978]),\n",
       " 'amid': tensor([-0.7702,  0.2121,  0.1505, -0.1471,  0.2019]),\n",
       " 'amidst': tensor([ 0.3470,  0.4200,  0.2646, -0.7514,  0.4112]),\n",
       " 'amin': tensor([-0.1814, -0.1799,  1.7316,  0.4766,  0.6524]),\n",
       " 'amini': tensor([ 0.8807, -0.3344,  0.1119,  1.3042, -2.0685]),\n",
       " 'amirkabir': tensor([ 1.1639,  0.4205, -1.5076, -1.1013,  0.3374]),\n",
       " 'amjad': tensor([-0.6329, -0.7900,  0.1788, -0.0009, -0.1907]),\n",
       " 'amlo': tensor([ 1.8155, -0.3232, -0.1771,  0.2046, -1.2888]),\n",
       " 'amma': tensor([-0.3677,  0.9849, -1.1515, -0.0792, -0.6580]),\n",
       " 'amnesty': tensor([ 1.4540, -1.0344,  0.3099, -1.2283,  0.0520]),\n",
       " 'amnok': tensor([ 0.1010, -0.5767, -0.8498,  0.2441,  0.3314]),\n",
       " 'among': tensor([ 0.4561,  1.1678,  0.2146, -0.0736, -0.4798]),\n",
       " 'amongst': tensor([-0.4602, -0.0449,  0.7763, -0.6974, -1.4238]),\n",
       " 'amordād': tensor([-0.3109,  0.3228, -0.6736,  2.3214,  1.0575]),\n",
       " 'amount': tensor([ 1.5013,  0.9024, -0.3859, -0.2103, -1.2174]),\n",
       " 'amounted': tensor([ 1.4369,  1.4932, -0.0833,  0.1232,  0.6496]),\n",
       " 'amounting': tensor([ 1.7369, -0.6410,  0.5334,  0.1612, -0.8214]),\n",
       " 'amounts': tensor([ 1.4159, -0.5180,  0.3609, -0.5652,  0.6449]),\n",
       " 'ampat': tensor([ 0.1326,  1.1199,  1.0846,  0.0169, -0.2459]),\n",
       " 'amphibian': tensor([ 2.0165,  0.8966, -2.9864,  0.4079,  0.0809]),\n",
       " 'amphibians': tensor([ 0.1390, -0.2876, -0.7835,  0.2086,  0.1478]),\n",
       " 'ample': tensor([ 0.4346, -0.6306, -0.3496, -0.7586, -0.2611]),\n",
       " 'amplified': tensor([-0.5065, -0.1947,  0.2486, -1.0648,  0.4985]),\n",
       " 'amputation': tensor([-0.5968, -0.0477, -0.6814, -1.4817,  0.5578]),\n",
       " 'amr': tensor([-1.1667,  0.3150, -0.0672, -0.4231, -0.2891]),\n",
       " 'amsterdam': tensor([-2.0805, -0.6211,  0.6810, -0.1951,  1.4374]),\n",
       " 'amur': tensor([-0.8853, -0.5856, -0.6734, -0.7031, -0.1379]),\n",
       " 'amusement': tensor([ 0.1374,  1.0759, -0.9152, -0.1651,  1.0767]),\n",
       " 'amy': tensor([-0.8797,  0.0501, -0.7359,  0.6181,  1.0335]),\n",
       " 'américa': tensor([ 1.4674,  0.1796, -0.0547,  0.2385, -0.0262]),\n",
       " 'américo': tensor([-0.5282,  0.0726, -0.9854, -0.4682,  0.2645]),\n",
       " 'amərətāt': tensor([-0.3636,  1.6196,  1.0274,  2.4644,  0.9388]),\n",
       " 'an': tensor([ 1.0273,  0.1868,  0.7194, -0.3109, -1.1238]),\n",
       " 'anadolu': tensor([-0.2372,  1.6939, -0.7740, -0.4006,  0.8961]),\n",
       " 'anahita': tensor([ 0.4349,  0.5468, -0.4059,  0.6568,  0.8831]),\n",
       " 'analog': tensor([ 1.7483, -1.5032,  1.1086,  0.1728,  0.7726]),\n",
       " 'analyse': tensor([-1.7095,  0.3409, -2.5335, -0.7806,  0.8775]),\n",
       " 'analysed': tensor([-0.6648,  0.9140,  0.1664, -0.3519,  1.5557]),\n",
       " 'analysis': tensor([ 0.2200,  0.2329,  0.2520, -0.1397, -0.1417]),\n",
       " 'analysts': tensor([-0.1359, -0.1235,  1.1250, -0.5109, -1.1702]),\n",
       " 'analytic': tensor([ 0.1510, -0.4190,  0.7328,  0.1283, -1.0933]),\n",
       " 'analytical': tensor([ 0.0088, -0.7083,  0.4259, -0.5618,  0.7909]),\n",
       " 'analyzed': tensor([ 1.4536,  0.4768, -0.1830,  0.6464, -0.2206]),\n",
       " 'analyzing': tensor([-0.4078, -0.3819, -0.2988, -1.2624, -0.5529]),\n",
       " 'anambra': tensor([ 0.4179,  0.3521, -2.0203,  0.0347,  1.2319]),\n",
       " 'anand': tensor([ 0.2616,  0.0943, -0.6367, -0.5735, -0.7444]),\n",
       " 'ananta': tensor([-2.2453, -0.7662,  1.7167,  0.6064,  0.0651]),\n",
       " 'anarchism': tensor([ 0.1074, -0.7136,  0.9662,  1.2760, -1.4138]),\n",
       " 'anastasianism': tensor([-1.1614, -1.4844, -1.3115, -0.9082,  0.2897]),\n",
       " 'anastasio': tensor([-0.0459, -0.5032, -0.3993, -1.1546, -0.1122]),\n",
       " 'anatole': tensor([-1.0353,  1.9469, -0.2466,  0.4302,  0.7161]),\n",
       " 'anatolia': tensor([-0.5906, -0.8938, -0.3301,  0.2879, -0.5446]),\n",
       " 'anatolian': tensor([-1.9247,  0.2853,  0.4134, -0.6605, -0.3272]),\n",
       " 'anatomically': tensor([-0.5930, -0.0770,  0.9220,  0.0625, -0.0118]),\n",
       " 'anatomy': tensor([ 0.1286,  2.2024, -1.5086,  2.0726,  0.1468]),\n",
       " 'ancestor': tensor([-0.6152,  1.7100, -1.4170, -0.3078,  0.3441]),\n",
       " 'ancestors': tensor([-1.4588, -1.1555, -0.4541, -0.3410, -0.1412]),\n",
       " 'ancestral': tensor([ 0.2770,  1.1188,  1.2292,  1.1032, -0.8533]),\n",
       " 'ancestries': tensor([-0.4215,  0.7207,  0.4912,  0.8032, -1.5817]),\n",
       " 'ancestry': tensor([ 0.4713, -0.3371,  0.4855, -0.2783, -1.0071]),\n",
       " 'anchors': tensor([-0.2079, -0.6781,  1.0739,  0.0224, -1.9845]),\n",
       " 'ancien': tensor([-0.5517,  1.2535, -1.8203, -0.9033,  0.0770]),\n",
       " 'ancient': tensor([-1.0704,  0.5848,  0.5154, -0.0699,  0.3674]),\n",
       " 'ancillary': tensor([ 0.6531, -0.8343, -0.1838, -0.9977, -0.3103]),\n",
       " 'ancona': tensor([ 0.7188,  0.6389, -0.6974,  0.8308,  1.2854]),\n",
       " 'ancus': tensor([-1.2524, -0.6101,  1.1355,  1.4398, -1.9476]),\n",
       " 'and': tensor([-0.1417, -0.3019,  1.1123, -0.5080,  0.4567]),\n",
       " 'andalusian': tensor([-0.2855, -2.0158, -0.1499, -0.1907,  1.8233]),\n",
       " 'andaman': tensor([-0.2234,  0.2962,  0.3057, -0.6284, -1.1180]),\n",
       " 'anderen': tensor([-8.0241e-01,  7.4154e-01,  1.2391e-02, -1.9457e-04,  1.6266e+00]),\n",
       " 'anderson': tensor([ 0.3247, -0.3135,  0.6037, -0.0605, -0.3389]),\n",
       " 'andhra': tensor([-1.4567,  0.3825,  0.3206,  1.6094, -0.6050]),\n",
       " 'andorra': tensor([ 0.7921, -0.5127, -0.6362,  0.3087, -0.4504]),\n",
       " 'andrade': tensor([-1.6219, -0.0833,  0.0475, -0.8427, -1.6988]),\n",
       " 'andrea': tensor([-0.4937,  0.5297,  0.2368,  0.7901, -0.5198]),\n",
       " 'andreas': tensor([ 0.2640,  0.9797, -1.4958,  0.8781, -0.5186]),\n",
       " 'andrei': tensor([-1.3060, -1.0453,  0.2777,  0.5655, -1.2334]),\n",
       " 'andreu': tensor([-1.5077, -0.2949, -2.1996, -1.4015,  0.5219]),\n",
       " 'andrew': tensor([ 0.0598, -0.6994, -0.8065,  1.3232, -0.1394]),\n",
       " 'andrey': tensor([-1.1002,  0.5561, -1.7343, -0.4102, -1.6495]),\n",
       " 'andreyev': tensor([-1.1550,  0.3975,  0.4212,  1.4635,  0.4227]),\n",
       " 'android': tensor([ 0.4820,  1.0585, -0.2236, -0.8955, -0.9408]),\n",
       " 'andrzej': tensor([-1.9145, -0.0603, -0.3981,  1.8393, -0.2463]),\n",
       " 'andré': tensor([-0.2531, -0.0548,  1.9990,  0.2611, -0.4872]),\n",
       " 'andrés': tensor([-0.0703, -0.2236, -0.6289,  1.3409, -0.2662]),\n",
       " 'andy': tensor([-0.1086, -0.8837,  0.3222,  0.3024, -0.1577]),\n",
       " 'anemia': tensor([-0.7402,  1.1586, -0.3698,  3.2698, -0.8394]),\n",
       " 'angara': tensor([ 1.4758, -0.1328, -1.3635, -1.3711,  0.8908]),\n",
       " 'angela': tensor([-0.6111, -0.7227, -0.5123, -0.6364, -0.4855]),\n",
       " 'angeles': tensor([ 0.6605,  0.4236, -0.6636,  0.3470, -0.5524]),\n",
       " 'angelico': tensor([-0.1263,  0.1259, -0.3767,  0.7789, -0.5439]),\n",
       " 'angell': tensor([ 0.3635, -0.0342, -1.1270,  2.2128,  1.3116]),\n",
       " 'angelo': tensor([-0.9189, -1.3581,  0.6035,  0.4687,  0.6578]),\n",
       " 'anger': tensor([-0.8796, -0.1214, -0.9011, -2.4186, -1.1739]),\n",
       " 'angered': tensor([ 0.2721,  0.6955,  0.3883, -0.0906, -0.5335]),\n",
       " 'anghiari': tensor([ 0.6337, -0.0187,  0.7697,  0.5602, -1.1186]),\n",
       " 'angklung': tensor([-0.9263, -0.5982, -1.5763,  1.1321,  0.0721]),\n",
       " 'angkot': tensor([ 0.0131,  1.1835, -0.4347,  1.0960,  1.0513]),\n",
       " 'anglesey': tensor([ 1.7233, -0.7668, -2.0177,  0.7394, -0.2685]),\n",
       " 'anglican': tensor([-0.2992, -0.7248,  1.4607, -0.3365,  0.3036]),\n",
       " 'anglicisation': tensor([-1.2145,  0.9671,  0.6231,  0.7449, -0.4305]),\n",
       " 'anglicised': tensor([-0.7741, -1.4955, -0.9346, -0.9505, -0.5639]),\n",
       " 'anglophone': tensor([ 1.7596,  0.9771, -1.2714, -1.0019,  0.2326]),\n",
       " 'anglosphere': tensor([ 0.0217,  0.4458, -1.5650, -0.0652, -1.2385]),\n",
       " 'angola': tensor([-0.9125,  0.1226, -1.0890, -0.5442, -0.8002]),\n",
       " 'angora': tensor([ 0.2893, -1.1931,  0.8846, -1.0012,  0.1971]),\n",
       " 'angry': tensor([-2.0012,  1.3596, -0.5829,  0.0815,  0.1308]),\n",
       " 'anguilla': tensor([-0.2333, -0.8910, -1.0204,  0.1649,  1.2395]),\n",
       " 'angular': tensor([ 0.4042,  0.4132, -0.5923, -0.1897,  1.3050]),\n",
       " 'anhui': tensor([ 0.2785,  0.5221, -1.6065, -0.6936, -1.0207]),\n",
       " 'aniceto': tensor([-0.2400,  2.3548,  0.2110,  0.1699,  0.3284]),\n",
       " 'animal': tensor([ 0.0103,  0.4833, -1.4143, -0.5718, -0.1766]),\n",
       " 'animals': tensor([ 0.0272, -0.4265, -0.9074,  0.1369, -0.0672]),\n",
       " 'animated': tensor([-0.3593, -0.5017,  1.6749, -1.1997,  0.1221]),\n",
       " 'animation': tensor([-0.3372, -0.0097, -0.1872, -0.1627,  0.1440]),\n",
       " 'anime': tensor([-0.3836, -0.2976, -0.7432,  0.5024,  2.0722]),\n",
       " 'animism': tensor([-0.6604, -0.9546,  0.8007,  0.6845, -1.0417]),\n",
       " 'animist': tensor([0.7438, 0.5013, 0.4557, 0.7870, 0.1118]),\n",
       " 'animosity': tensor([-0.3378,  0.8512, -1.2335, -0.4925, -0.0739]),\n",
       " 'anish': tensor([ 0.8681, -0.1398,  1.3066,  1.5323,  1.6953]),\n",
       " 'anita': tensor([-1.2944,  1.8851, -1.2311,  1.8838,  0.0096]),\n",
       " 'anjou': tensor([-1.6920,  1.1820, -0.5594, -0.4016, -0.0492]),\n",
       " 'ankara': tensor([ 2.1099, -0.6694,  0.0812, -0.5791,  1.2206]),\n",
       " 'ankón': tensor([-1.2309,  1.6517,  0.0084,  0.5807,  1.4131]),\n",
       " 'anna': tensor([ 0.2937, -1.2866,  0.3089,  0.5027,  1.7652]),\n",
       " 'annazids': tensor([-1.5658,  2.3284, -0.0346,  0.8956,  0.5414]),\n",
       " 'anne': tensor([-0.4632,  0.4782, -0.9350,  1.3107, -1.7200]),\n",
       " 'annex': tensor([-0.0363,  0.0851, -1.2241, -0.9514,  0.6670]),\n",
       " 'annexation': tensor([-0.7782, -1.2180,  0.3613,  0.0471,  0.3503]),\n",
       " 'annexations': tensor([-0.5135, -1.1820, -0.4773, -0.1314,  1.0620]),\n",
       " 'annexe': tensor([-0.2925,  0.2702,  0.5024, -0.3829, -0.5258]),\n",
       " 'annexed': tensor([-0.5434, -0.2992, -1.2524, -0.1118,  0.0677]),\n",
       " 'annexing': tensor([ 0.4392, -1.7508,  0.4614, -0.7331,  0.6471]),\n",
       " 'annibale': tensor([-1.0050,  0.0111, -0.9806,  1.6914, -0.7804]),\n",
       " 'annihilated': tensor([-2.1395,  0.3591, -1.1381, -1.0175, -0.2738]),\n",
       " 'annihilation': tensor([-0.7933,  1.1852,  0.4195,  0.4000,  1.4550]),\n",
       " 'anniversary': tensor([-1.3983,  0.4211, -0.4799, -1.9387,  0.1426]),\n",
       " 'announced': tensor([ 0.7659,  0.5893, -0.3452,  0.2644,  0.5891]),\n",
       " 'announcement': tensor([-0.8534, -0.7631, -0.2631,  0.9975, -0.7980]),\n",
       " 'announcements': tensor([ 0.0017,  1.0048,  0.9839, -0.2651,  0.9488]),\n",
       " 'announcing': tensor([-2.3517,  1.4951, -0.4333, -0.5455,  1.7814]),\n",
       " 'annual': tensor([ 0.4445,  0.4419,  0.4324,  0.1838, -0.9024]),\n",
       " 'annualised': tensor([-0.3549,  0.3323,  0.3419, -0.4211,  0.6813]),\n",
       " 'annually': tensor([-0.5476,  0.4714, -0.0172, -0.1622, -0.7051]),\n",
       " 'annulled': tensor([ 1.4279, -0.9579, -0.1767,  0.6939,  0.6919]),\n",
       " 'anointing': tensor([-1.4355, -0.3300, -0.1383,  0.1830,  2.2495]),\n",
       " 'anonymous': tensor([-1.2741, -0.6593,  0.0198, -0.5031, -0.9885]),\n",
       " 'another': tensor([-1.6456,  0.1890, -0.4731,  0.0737, -0.1274]),\n",
       " 'ansel': tensor([0.3104, 0.7669, 0.2394, 0.5099, 0.6840]),\n",
       " 'answer': tensor([ 0.7429, -0.6721, -1.1599, -0.8244, -0.4646]),\n",
       " 'antakya': tensor([ 0.8512,  1.0899, -1.0449,  0.0624,  1.0113]),\n",
       " 'antalya': tensor([-1.6344, -0.1336,  0.3151, -0.1433, -0.6183]),\n",
       " 'antarctic': tensor([ 0.7423,  0.8711, -0.2145, -0.9007,  0.0733]),\n",
       " 'antarctica': tensor([ 0.8779,  0.1588,  0.8078, -0.1191, -0.4411]),\n",
       " 'antariksa': tensor([-0.4410,  0.5323,  0.1070,  1.7675,  1.6613]),\n",
       " 'anteaters': tensor([-2.0567,  0.3852, -0.5186,  0.6100,  1.8214]),\n",
       " 'antecedents': tensor([ 1.1399,  0.1723, -0.5537, -0.1371,  0.4063]),\n",
       " 'anthem': tensor([ 0.2349, -0.7704, -0.6385,  0.0265,  0.4067]),\n",
       " 'anthems': tensor([-0.6814,  0.8592,  2.6767,  0.2760,  0.6906]),\n",
       " 'anthology': tensor([ 1.2025, -0.4288,  0.0458, -0.3205, -1.8807]),\n",
       " 'anthony': tensor([ 0.0618,  2.4570, -1.1154,  0.5781,  0.1588]),\n",
       " 'antibiotics': tensor([-0.6110,  0.1954, -0.7737,  0.5362, -1.1313]),\n",
       " 'anticipating': tensor([-0.2450,  0.0675, -0.8597,  0.6324, -1.3349]),\n",
       " 'anticlerical': tensor([-1.0786, -2.7098, -0.0959, -0.0678, -0.4616]),\n",
       " 'anticline': tensor([-1.3378, -0.6871,  0.1336,  0.4087, -0.1085]),\n",
       " 'antinous': tensor([ 1.1071,  0.7149, -1.5644,  0.5144, -0.0920]),\n",
       " 'antioch': tensor([-1.1671, -0.1555, -1.0144,  0.5903, -0.9858]),\n",
       " 'antiochian': tensor([-0.8539, -0.1375, -1.8833,  2.2580,  0.0307]),\n",
       " 'antiochus': tensor([-2.3095,  0.9543,  0.5472, -1.1610, -2.2831]),\n",
       " 'antiproton': tensor([-2.4240,  0.4730, -0.7539,  2.7180, -1.6955]),\n",
       " 'antiquated': tensor([ 0.3776, -2.0833,  0.6793, -0.5936, -0.5498]),\n",
       " 'antiquities': tensor([ 0.3914,  0.5028,  1.2690, -0.6490, -0.3538]),\n",
       " 'antiquity': tensor([-0.2898,  0.0425, -0.6124, -0.4796, -0.8279]),\n",
       " 'antireligious': tensor([-0.8431, -0.3132, -0.2691, -1.1523, -0.3608]),\n",
       " 'antisemitic': tensor([-1.2939,  1.0939,  0.7611,  1.8542, -1.1016]),\n",
       " 'antisemitism': tensor([-0.2435, -0.5909, -0.4220,  1.3602, -0.9205]),\n",
       " 'antoine': tensor([-0.0137, -0.0473,  0.5602, -0.3546, -0.6682]),\n",
       " 'anton': tensor([-0.6780,  0.4556, -0.5423, -1.1567,  2.4927]),\n",
       " 'antonio': tensor([-0.3267, -0.8283,  0.3917,  1.0718, -0.5244]),\n",
       " 'antonioni': tensor([-0.3126, -0.6750,  0.0050,  0.4905, -0.2919]),\n",
       " 'antony': tensor([-3.2781,  1.5264, -0.1377,  0.9558,  0.2538]),\n",
       " 'antônio': tensor([-0.2894,  0.6792, -1.3562,  0.1500, -1.2762]),\n",
       " 'anwar': tensor([ 0.8830, -0.0407,  1.4066,  0.4601, -0.3615]),\n",
       " 'any': tensor([ 0.0746,  1.6997, -0.1046, -0.7353, -0.1557]),\n",
       " 'anyaene': tensor([-0.1263, -0.8392,  0.9060,  1.0508,  0.3367]),\n",
       " 'anyang': tensor([-1.3512,  0.3043,  0.3275,  0.9605,  0.2449]),\n",
       " 'anymore': tensor([ 0.2066,  0.5694,  0.3430, -0.1695,  0.9966]),\n",
       " 'anyone': tensor([-0.8525,  1.0963, -1.2590,  0.0080,  0.9140]),\n",
       " 'anything': tensor([ 0.0230, -0.1437, -0.7006,  0.5033,  0.3455]),\n",
       " 'anywhere': tensor([ 2.6498,  0.9719, -0.5551,  0.5064, -0.9987]),\n",
       " 'anzac': tensor([-2.0457,  0.3427,  0.4057, -1.5887, -0.3324]),\n",
       " 'anzacs': tensor([1.1543, 0.9493, 0.1586, 0.1663, 1.9903]),\n",
       " 'anzali': tensor([-0.3647,  1.1635,  0.3302,  0.7558, -0.7538]),\n",
       " 'anzus': tensor([ 2.8437e-01, -1.9135e-03, -1.4495e+00, -8.7847e-01,  2.0523e+00]),\n",
       " 'anáhuac': tensor([-0.5717,  1.8722,  0.0150,  0.7679,  0.1851]),\n",
       " 'aoc': tensor([-0.5321,  1.1651,  1.1387, -0.4409, -0.7691]),\n",
       " 'aosta': tensor([ 0.4822,  0.2829, -0.3197,  0.1460, -1.3834]),\n",
       " 'ap': tensor([ 0.4504, -0.2368,  1.5060,  0.0773, -0.2937]),\n",
       " 'apa': tensor([-0.3720,  1.6092,  0.9076,  0.5545, -1.8231]),\n",
       " 'apadana': tensor([-0.3524, -0.3300,  0.4175, -1.2009,  0.6080]),\n",
       " 'apart': tensor([-0.5636, -0.7682,  0.7177, -0.2540, -0.0465]),\n",
       " 'apartheid': tensor([-2.8885, -1.0378, -1.2443, -0.1286,  1.4048]),\n",
       " 'apec': tensor([ 1.3307,  0.1524, -0.1283, -0.4843, -1.5421]),\n",
       " 'apennine': tensor([ 0.5353,  0.9275,  0.1833, -1.1403, -0.1577]),\n",
       " 'apennines': tensor([-0.8261,  0.2294,  0.0564,  0.0549,  0.7984]),\n",
       " 'apex': tensor([-1.8701,  0.8366,  0.4618,  0.3566,  0.8872]),\n",
       " 'aphrodisias': tensor([-1.1551, -0.6753, -1.1461,  1.6962,  0.0460]),\n",
       " 'apocalypse': tensor([-0.4823, -1.4577,  0.2183, -0.3866,  0.7655]),\n",
       " 'apogee': tensor([ 0.4426, -0.6585,  0.9980, -1.5266,  1.3406]),\n",
       " 'apollo': tensor([ 0.6032,  0.6495, -1.0079, -0.2252, -0.0586]),\n",
       " 'apostasy': tensor([ 1.1335,  0.1593, -1.3415,  0.8096,  0.7368]),\n",
       " 'apostle': tensor([ 0.9946,  0.8696,  0.0398, -1.4406,  0.2260]),\n",
       " 'apostles': tensor([ 0.7468,  0.5147, -0.2963, -0.9902,  1.2076]),\n",
       " 'appalachian': tensor([0.6887, 0.3892, 0.5605, 0.0870, 0.7894]),\n",
       " 'apparatus': tensor([ 1.3997,  0.8423,  0.5513, -0.5131,  1.9456]),\n",
       " 'apparel': tensor([-1.2688,  1.2064,  0.7167, -0.4587, -0.2672]),\n",
       " 'apparent': tensor([-0.0754,  0.0593,  0.3630, -0.8656, -0.0143]),\n",
       " 'apparently': tensor([-0.5328, -1.1137, -0.5694,  2.7200,  0.0371]),\n",
       " 'appeal': tensor([ 0.2640, -1.4413, -1.5102, -0.6611,  0.1057]),\n",
       " 'appealed': tensor([-1.0426,  0.4603,  0.6050, -0.1055,  2.3633]),\n",
       " 'appeals': tensor([ 0.2623, -0.0704,  0.8797,  0.6730,  0.0915]),\n",
       " 'appear': tensor([ 1.5745,  1.1636, -0.8642,  0.1001,  0.9078]),\n",
       " 'appearance': tensor([-0.3744, -0.4382,  0.0029, -0.4061, -0.2381]),\n",
       " 'appearances': tensor([-0.3782,  0.2034,  1.0191, -1.0070,  0.4078]),\n",
       " 'appeared': tensor([-0.4769,  0.0051,  0.3329, -0.0041, -0.2522]),\n",
       " 'appearing': tensor([ 1.1896, -0.7708, -0.4977,  0.9379,  0.9929]),\n",
       " 'appears': tensor([ 0.5844,  0.3213, -1.2449,  0.4186,  1.2704]),\n",
       " 'appellate': tensor([ 0.5812, -0.6009,  0.6685,  0.5515,  0.8138]),\n",
       " 'appellation': tensor([-0.1165, -0.9831,  0.2919, -0.6969,  0.6272]),\n",
       " 'applauded': tensor([ 1.0772, -0.6623,  0.2791, -1.1629,  0.9905]),\n",
       " 'apple': tensor([-0.4154,  0.0502, -0.1787, -1.1250, -0.1651]),\n",
       " 'apples': tensor([-0.5567,  0.9267, -1.1420,  0.8779, -0.2646]),\n",
       " 'appliances': tensor([-0.0884,  0.1423, -0.4313, -0.4630, -1.0139]),\n",
       " 'applicable': tensor([ 1.1439,  0.4869, -1.1592, -0.0455,  0.5762]),\n",
       " 'application': tensor([-0.3279,  0.5740, -0.3838, -0.5568,  0.5674]),\n",
       " 'applications': tensor([ 1.7482, -0.3479,  1.1671,  0.3026,  0.4773]),\n",
       " 'applied': tensor([-0.1863,  1.3589, -0.4648, -0.3665,  0.0857]),\n",
       " 'applies': tensor([ 1.3516, -0.0054, -0.1165,  0.8053, -0.0535]),\n",
       " 'appliques': tensor([-0.8009, -0.0670,  0.3130,  0.6695,  0.2004]),\n",
       " 'apply': tensor([-0.0610,  1.6729, -1.5770, -0.4195,  0.3323]),\n",
       " 'appoint': tensor([-0.3566, -0.1191, -0.9761, -0.5848,  1.8043]),\n",
       " 'appointed': tensor([-0.6877, -0.2959, -0.0760, -0.0641,  2.6676]),\n",
       " 'appointees': tensor([ 2.4735,  0.3104, -0.4709,  2.1702, -1.9318]),\n",
       " 'appointing': tensor([0.6707, 0.0273, 1.2358, 0.2074, 0.2746]),\n",
       " 'appointive': tensor([-0.7434, -1.0245,  2.1745, -0.4403, -0.1652]),\n",
       " 'appointment': tensor([-1.2394,  2.4991, -0.1569,  0.4599,  0.9102]),\n",
       " 'appointments': tensor([-1.0656,  0.2764,  0.5289, -0.4182,  1.9739]),\n",
       " 'appoints': tensor([ 1.1978, -0.8718, -0.5805, -1.1156,  0.7100]),\n",
       " 'appomattox': tensor([ 0.7945,  0.0877, -0.0737, -0.8502,  1.7501]),\n",
       " 'apportioned': tensor([ 0.0320, -2.1658, -0.3059,  0.4448, -0.1486]),\n",
       " 'apportionment': tensor([-0.2174, -2.2787,  0.7124,  0.3884, -1.3989]),\n",
       " 'appreciable': tensor([-0.5278,  0.8355,  0.2474,  0.4835,  2.4429]),\n",
       " 'appreciated': tensor([ 0.4245, -0.4163, -0.4813,  0.7764, -1.2975]),\n",
       " 'appreciative': tensor([-0.4757,  0.1772,  0.4397, -0.8653,  0.8097]),\n",
       " 'apprentice': tensor([-1.5657,  1.5143, -0.3347, -0.4064,  0.0606]),\n",
       " 'apprenticeship': tensor([-1.1167, -0.9311,  1.0016,  0.8562,  0.8540]),\n",
       " 'apprenticeships': tensor([ 0.0712,  1.4182, -0.5206, -0.6874,  0.0666]),\n",
       " 'approach': tensor([ 1.3294, -0.6559, -1.9983, -0.1336,  1.0276]),\n",
       " 'approached': tensor([-0.6671,  0.2340, -1.6555,  0.1563,  0.1097]),\n",
       " 'approaches': tensor([ 0.3203, -1.2365, -0.6603, -0.3541,  0.7424]),\n",
       " 'approaching': tensor([ 0.5432, -1.5855,  0.3008, -1.5578, -1.5503]),\n",
       " 'appropriate': tensor([-0.7498, -0.5583,  0.5363,  0.3179,  1.4612]),\n",
       " 'approval': tensor([ 1.3588, -0.0673,  1.5227, -0.0288,  1.1423]),\n",
       " 'approved': tensor([-0.0202, -0.2822,  0.3709, -0.3810,  0.9276]),\n",
       " 'approves': tensor([ 1.8778, -0.0525, -0.3556,  0.5950, -0.0436]),\n",
       " 'approving': tensor([ 0.6308,  0.1624,  0.5107, -0.8929,  0.0985]),\n",
       " 'approximate': tensor([0.3618, 0.9638, 0.3605, 0.4675, 0.4929]),\n",
       " 'approximated': tensor([-0.2200, -1.3499,  0.7511, -0.4542,  1.9304]),\n",
       " 'approximately': tensor([ 1.0160,  0.1525, -0.4122, -0.3177, -0.7519]),\n",
       " 'apps': tensor([-0.1359,  0.4709, -1.2042,  0.4499, -0.0133]),\n",
       " 'apricots': tensor([-0.3957, -0.3264,  3.2855,  0.1948, -1.2624]),\n",
       " 'april': tensor([-0.1947,  0.3411,  0.4543, -0.6866,  1.5493]),\n",
       " 'aqaba': tensor([-0.8744,  0.6556,  0.5340, -0.4991, -1.3651]),\n",
       " 'aquamarine': tensor([-1.5013,  0.8190,  1.6542,  1.1378, -0.3359]),\n",
       " 'aquarium': tensor([-0.9944,  0.4903,  0.1653,  0.3641,  0.9513]),\n",
       " ...}"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baháʼí', tensor(0.9973)),\n",
       " ('geometrical', tensor(0.9928)),\n",
       " ('guerrillas', tensor(0.9926)),\n",
       " ('anointing', tensor(0.9870)),\n",
       " ('led', tensor(0.9868))]"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar('president', word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7967)"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_dict['king'], word_dict['queen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.4596)"
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(word_dict['king'], word_dict['car'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
